{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/watsonselah/bubba-watson/blob/master/pyspark_postgres_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QbPsmEf6ljt"
      },
      "source": [
        "# Purpose\n",
        "\n",
        "Explore PySpark and the JDBC connection functionality to read from operational databases.\n",
        "\n",
        "In this notebook we will setup a PostgreSQL instance and populate it with the Pagila dataset. We will then connect to the database via a JDBC connector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-RHL4bg4u0_"
      },
      "source": [
        "# Setup\n",
        "\n",
        "## PostgreSQL\n",
        "\n",
        "Firstly, let's install postgres in the this Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhmGVh22JcNo",
        "outputId": "41a31168-7162-467e-9303-357165ca6372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n",
            "  logrotate netbase postgresql-14 postgresql-client-14\n",
            "  postgresql-client-common postgresql-common ssl-cert sysstat\n",
            "Suggested packages:\n",
            "  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n",
            "  logrotate netbase postgresql postgresql-14 postgresql-client-14\n",
            "  postgresql-client-common postgresql-common postgresql-contrib ssl-cert\n",
            "  sysstat\n",
            "0 upgraded, 14 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 18.5 MB of archives.\n",
            "After this operation, 52.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libjson-xs-perl amd64 4.040-0ubuntu0.22.04.1 [87.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.19-0ubuntu0.22.04.1 [1,249 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.19-0ubuntu0.22.04.1 [16.2 MB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Fetched 18.5 MB in 1s (14.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 14.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package logrotate.\n",
            "(Reading database ... 126666 files and directories currently installed.)\n",
            "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../05-libjson-xs-perl_4.040-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../06-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../07-postgresql-client-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../08-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../09-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../10-postgresql-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../11-postgresql_14+238_all.deb ...\n",
            "Unpacking postgresql (14+238) ...\n",
            "Selecting previously unselected package postgresql-contrib.\n",
            "Preparing to unpack .../12-postgresql-contrib_14+238_all.deb ...\n",
            "Unpacking postgresql-contrib (14+238) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../13-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Setting up postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up postgresql-common (238) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql-contrib (14+238) ...\n",
            "Setting up postgresql (14+238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install postgresql postgresql-contrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ajhL0Z_-KK8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12af933-79a9-4c29-ce0d-dba6b086ee72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n"
          ]
        }
      ],
      "source": [
        "!service postgresql start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P48P8Vt6Fm9"
      },
      "source": [
        "Create a user in Postgres ([stackoverflow](https://stackoverflow.com/questions/12720967/how-to-change-postgresql-user-password/12721020#12721020))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "b25UVuzVNdKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09591f8f-1cc7-488f-de77-20d0f9c515a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALTER ROLE\n"
          ]
        }
      ],
      "source": [
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'test';\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW1kucySWAKv"
      },
      "source": [
        "Store you database password in an environmental variable so that we need no type it in all the time (not advisable generally).\n",
        "\n",
        "We'll use the notebook magic `%end`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "as0Zs9kL6PY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bf4b59-1f03-49c0-a666-6591d9679168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PGPASSWORD=test\n"
          ]
        }
      ],
      "source": [
        "%env PGPASSWORD=test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGqYbg366efu"
      },
      "source": [
        "## Pagila\n",
        "\n",
        "Now, let's populate the PostgreSQL database with the Pagila data from the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qICjoP_dKS8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cac5e97-6054-4dc9-b259-c819a8e9a22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pagila'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 94 (delta 20), reused 19 (delta 19), pack-reused 56 (from 1)\u001b[K\n",
            "Receiving objects: 100% (94/94), 2.89 MiB | 18.73 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/spatialedge-ai/pagila.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xYHVKYqSMthy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b76576-654e-49ce-9d50-50236a4dad68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATE DATABASE\n"
          ]
        }
      ],
      "source": [
        "!psql -h localhost -U postgres -c \"create database pagila\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kfgNogz3MSq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb86745b-6f64-4d4a-aa48-5b7ff40016b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            " set_config \n",
            "------------\n",
            " \n",
            "(1 row)\n",
            "\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "CREATE TYPE\n",
            "ALTER TYPE\n",
            "CREATE DOMAIN\n",
            "ALTER DOMAIN\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "SET\n",
            "SET\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE FUNCTION\n",
            "ALTER FUNCTION\n",
            "CREATE AGGREGATE\n",
            "ALTER AGGREGATE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE SEQUENCE\n",
            "ALTER TABLE\n",
            "CREATE TABLE\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "CREATE VIEW\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "CREATE INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "ALTER INDEX\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "CREATE TRIGGER\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n",
            "ALTER TABLE\n"
          ]
        }
      ],
      "source": [
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-schema.sql\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8zpqaYNZPABo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee36b02-cc32-47b4-b259-95a5f55002d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            " set_config \n",
            "------------\n",
            " \n",
            "(1 row)\n",
            "\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "SET\n",
            "COPY 200\n",
            "COPY 109\n",
            "COPY 600\n",
            "COPY 603\n",
            "COPY 16\n",
            "COPY 2\n",
            "COPY 599\n",
            "COPY 6\n",
            "COPY 1000\n",
            "COPY 5462\n",
            "COPY 1000\n",
            "COPY 4581\n",
            "COPY 2\n",
            "COPY 16044\n",
            "COPY 1157\n",
            "COPY 2312\n",
            "COPY 5644\n",
            "COPY 6754\n",
            "COPY 182\n",
            "COPY 0\n",
            " setval \n",
            "--------\n",
            "    200\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    605\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "     16\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    600\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    109\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "    599\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "   1000\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "   4581\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      6\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "  32098\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "  16049\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      2\n",
            "(1 row)\n",
            "\n",
            " setval \n",
            "--------\n",
            "      2\n",
            "(1 row)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-data.sql\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M0a4GiI6yyr"
      },
      "source": [
        "## PySpark Setup\n",
        "\n",
        "Now, let's download what is necessary for initiating jdbc connections, as well as what is required to run PySpark itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bCiCzTg-Jx2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e87c61-37a7-4f38-df92-d95caec30b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-02 16:30:02--  https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1046274 (1022K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.5.0.jar’\n",
            "\n",
            "postgresql-42.5.0.j 100%[===================>]   1022K  1.66MB/s    in 0.6s    \n",
            "\n",
            "2025-10-02 16:30:04 (1.66 MB/s) - ‘postgresql-42.5.0.jar’ saved [1046274/1046274]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# https://stackoverflow.com/questions/34948296/using-pyspark-to-connect-to-postgresql\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.5.0.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2BQsxrwZBhWc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "%config Completer.use_jedi = False\n",
        "\n",
        "SPARKVERSION='3.2.1'\n",
        "HADOOPVERSION='3.2'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}\"\n",
        "\n",
        "# print(os.environ['SPARK_HOME'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1owkTgHVBuix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f577b4fd-8256-4c96-afc6-5d30dc9869ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "--2025-10-02 16:30:31--  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 300971569 (287M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.1-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.2.1-bin-had 100%[===================>] 287.03M   315KB/s    in 16m 57s \n",
            "\n",
            "2025-10-02 16:47:29 (289 KB/s) - ‘spark-3.2.1-bin-hadoop3.2.tgz’ saved [300971569/300971569]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://archive.apache.org/dist/spark/spark-{SPARKVERSION}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz\n",
        "!tar xf spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ighjc_WdUNgC"
      },
      "outputs": [],
      "source": [
        "!cp postgresql-42.5.0.jar spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8DvgX7OEHWo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gCIQhdSYC5uh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5cb271-0e66-4f84-e066-c0117188b05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "reMhwdxpCz05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45c6db0-157c-4b78-8abd-21a66af01f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "postgresql-42.2.5.jar\n",
            "env: PYARROW_IGNORE_TIMEZONE=1\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "# get a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.config(\"spark.jars\",\n",
        "                                                       \"postgresql-42.2.5.jar\").config(\n",
        "                                                          \"spark.driver.extraClassPath\",\n",
        "                                                          f\"spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\"\n",
        "                                                       ).getOrCreate()\n",
        "print(spark.conf.get('spark.jars'))\n",
        "\n",
        "%env PYARROW_IGNORE_TIMEZONE=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqG_Hk4YXuC7"
      },
      "source": [
        "# Questions\n",
        "\n",
        "### Question 1\n",
        "\n",
        "Using a PySpark dataframe, print the schema of customer table in the pagila PostgreSQL database by utilising a JDBC connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "EnrQk09jQyaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec360a4-3f41-4d98-a3ad-9349642a7e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema of customer table:\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- store_id: integer (nullable = true)\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- address_id: integer (nullable = true)\n",
            " |-- activebool: boolean (nullable = true)\n",
            " |-- create_date: date (nullable = true)\n",
            " |-- last_update: timestamp (nullable = true)\n",
            " |-- active: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# JDBC connection properties\n",
        "jdbc_url = \"jdbc:postgresql://localhost:5432/pagila\"\n",
        "connection_properties = {\n",
        "    \"user\": \"postgres\",\n",
        "    \"password\": \"test\",\n",
        "    \"driver\": \"org.postgresql.Driver\"\n",
        "}\n",
        "\n",
        "# Read customer table using JDBC\n",
        "customer_df = spark.read.jdbc(\n",
        "    url=jdbc_url,\n",
        "    table=\"customer\",\n",
        "    properties=connection_properties\n",
        ")\n",
        "\n",
        "# Print the schema\n",
        "print(\"Schema of customer table:\")\n",
        "customer_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXhnjaylCFI1"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "Use the Spark SQL API to query the customer table, compute the number of unique email addresses in that table and print the result in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xTGwAFhYpanl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cda651-47fb-4aa6-9775-6c71a09e9209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique email addresses (using Spark SQL):\n",
            "+------------------+\n",
            "|unique_email_count|\n",
            "+------------------+\n",
            "|               599|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a temporary view for SQL queries\n",
        "customer_df.createOrReplaceTempView(\"customer\")\n",
        "\n",
        "# Use Spark SQL to count unique email addresses\n",
        "unique_emails_sql = spark.sql(\"\"\"\n",
        "    SELECT COUNT(DISTINCT email) as unique_email_count\n",
        "    FROM customer\n",
        "    WHERE email IS NOT NULL\n",
        "\"\"\")\n",
        "\n",
        "print(\"Number of unique email addresses (using Spark SQL):\")\n",
        "unique_emails_sql.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg7To_5dCRGb"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "Repeat this calculation using only the Dataframe API and print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hTO78anmCa37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6936de-5007-4216-d08f-c227ec46f7bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique email addresses (using DataFrame API):\n",
            "+------------------+\n",
            "|unique_email_count|\n",
            "+------------------+\n",
            "|               599|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import countDistinct, col\n",
        "\n",
        "# Use DataFrame API to count unique email addresses\n",
        "unique_emails_df = customer_df.filter(col(\"email\").isNotNull()) \\\n",
        "                             .agg(countDistinct(\"email\").alias(\"unique_email_count\"))\n",
        "\n",
        "print(\"Number of unique email addresses (using DataFrame API):\")\n",
        "unique_emails_df.show()\n",
        "\n",
        "# Store this DataFrame for Question 4\n",
        "question3_df = unique_emails_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IIL4RDSCcn4"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "How many partitions are present in the dataframe resulting from Question 3 (additionally provide the code necessary to determine that)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of partitions\n",
        "num_partitions = question3_df.rdd.getNumPartitions()\n",
        "print(f\"Number of partitions in the DataFrame from Question 3: {num_partitions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k-Ntv42_rTx",
        "outputId": "f953bac4-6a3b-4b2c-bd08-e5e65f947e81"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions in the DataFrame from Question 3: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_6o4oLIC5SJ"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "Compute the min and max of customer.create_date and print the result (once more using the Spark DataFrame API and not the Spark SQL API)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min as spark_min, max as spark_max\n",
        "\n",
        "# Compute min and max of create_date using DataFrame API\n",
        "min_max_df = customer_df.agg(\n",
        "    spark_min(\"create_date\").alias(\"min_create_date\"),\n",
        "    spark_max(\"create_date\").alias(\"max_create_date\")\n",
        ")\n",
        "\n",
        "print(\"Min and Max of customer.create_date (using DataFrame API):\")\n",
        "min_max_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKc0womlABVJ",
        "outputId": "3c1f3fd3-106f-407a-ca23-d63f17bbbd9a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min and Max of customer.create_date (using DataFrame API):\n",
            "+---------------+---------------+\n",
            "|min_create_date|max_create_date|\n",
            "+---------------+---------------+\n",
            "|     2020-02-14|     2020-02-14|\n",
            "+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vndZmoyC-Ay"
      },
      "source": [
        "### Question 6.1\n",
        "\n",
        "Determine which first names occur more than once:\n",
        "\n",
        "1. using the Spark SQL API (printing the result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_names_sql = spark.sql(\"\"\"\n",
        "    SELECT first_name, COUNT(*) as name_count\n",
        "    FROM customer\n",
        "    WHERE first_name IS NOT NULL\n",
        "    GROUP BY first_name\n",
        "    HAVING COUNT(*) > 1\n",
        "    ORDER BY name_count DESC, first_name\n",
        "\"\"\")\n",
        "\n",
        "print(\"First names that occur more than once (using Spark SQL):\")\n",
        "duplicate_names_sql.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxntAJfzATcl",
        "outputId": "27340437-4eea-416e-f324-1a737688212f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First names that occur more than once (using Spark SQL):\n",
            "+----------+----------+\n",
            "|first_name|name_count|\n",
            "+----------+----------+\n",
            "|     JAMIE|         2|\n",
            "|    JESSIE|         2|\n",
            "|     KELLY|         2|\n",
            "|    LESLIE|         2|\n",
            "|    MARION|         2|\n",
            "|     TERRY|         2|\n",
            "|     TRACY|         2|\n",
            "|    WILLIE|         2|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-qGmjBqDErO"
      },
      "source": [
        "### Question 6.2\n",
        "\n",
        "  2. using the Spark Dataframe API (printing the result once more)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, desc\n",
        "\n",
        "duplicate_names_df = customer_df.filter(col(\"first_name\").isNotNull()) \\\n",
        "                                .groupBy(\"first_name\") \\\n",
        "                                .agg(count(\"*\").alias(\"name_count\")) \\\n",
        "                                .filter(col(\"name_count\") > 1) \\\n",
        "                                .orderBy(desc(\"name_count\"), \"first_name\")\n",
        "\n",
        "print(\"First names that occur more than once (using DataFrame API):\")\n",
        "duplicate_names_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn6OHHJcAcm2",
        "outputId": "df651485-174c-4399-b834-740df55534c8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First names that occur more than once (using DataFrame API):\n",
            "+----------+----------+\n",
            "|first_name|name_count|\n",
            "+----------+----------+\n",
            "|     JAMIE|         2|\n",
            "|    JESSIE|         2|\n",
            "|     KELLY|         2|\n",
            "|    LESLIE|         2|\n",
            "|    MARION|         2|\n",
            "|     TERRY|         2|\n",
            "|     TRACY|         2|\n",
            "|    WILLIE|         2|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA56WFXXDqrm"
      },
      "source": [
        "### Question 7\n",
        "\n",
        "Port the PostgreSQL below to the PySpark DataFrame API and execute the query within Spark (not directly on PostgreSQL):\n",
        "\n",
        "```\n",
        "SELECT\n",
        "   staff.first_name\n",
        "   ,staff.last_name\n",
        "   ,SUM(payment.amount)\n",
        " FROM payment\n",
        "   INNER JOIN staff ON payment.staff_id = staff.staff_id\n",
        " WHERE payment.payment_date BETWEEN '2007-01-01' AND '2020-02-01'\n",
        " GROUP BY\n",
        "   staff.last_name\n",
        "   ,staff.first_name\n",
        " ORDER BY SUM(payment.amount)\n",
        " ;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read payment and staff tables\n",
        "payment_df = spark.read.jdbc(\n",
        "    url=jdbc_url,\n",
        "    table=\"payment\",\n",
        "    properties=connection_properties\n",
        ")\n",
        "\n",
        "staff_df = spark.read.jdbc(\n",
        "    url=jdbc_url,\n",
        "    table=\"staff\",\n",
        "    properties=connection_properties\n",
        ")\n",
        "\n",
        "# Port the PostgreSQL query to DataFrame API\n",
        "q7_result_df = payment_df.join(\n",
        "    staff_df,\n",
        "    payment_df.staff_id == staff_df.staff_id,\n",
        "    \"inner\"\n",
        ").filter(\n",
        "    (col(\"payment_date\") >= \"2007-01-01\") &\n",
        "    (col(\"payment_date\") <= \"2020-02-01\")\n",
        ").groupBy(\n",
        "    staff_df.last_name,\n",
        "    staff_df.first_name\n",
        ").agg(\n",
        "    spark_sum(\"amount\").alias(\"total_amount\")\n",
        ").orderBy(\n",
        "    \"total_amount\"\n",
        ")\n",
        "\n",
        "print(\"PostgreSQL query ported to PySpark DataFrame API:\")\n",
        "q7_result_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl8t7wM1AqAv",
        "outputId": "18d530a0-a536-4664-a2dd-8ed5e190a64a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PostgreSQL query ported to PySpark DataFrame API:\n",
            "+---------+----------+------------+\n",
            "|last_name|first_name|total_amount|\n",
            "+---------+----------+------------+\n",
            "| Stephens|       Jon|     2202.60|\n",
            "|  Hillyer|      Mike|     2621.83|\n",
            "+---------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqv7FoidJiBJ"
      },
      "source": [
        "### Question 8\n",
        "\n",
        "Are you currently executing commands on a driver node, or a worker? Provide the code you ran to determine that."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\n",
        "from pyspark import TaskContext\n",
        "\n",
        "# Method 1: Check if we can access SparkContext directly (driver only)\n",
        "try:\n",
        "    sc = spark.sparkContext\n",
        "    print(f\"SparkContext accessible: {sc is not None}\")\n",
        "    print(f\"Application ID: {sc.applicationId}\")\n",
        "    print(f\"Master URL: {sc.master}\")\n",
        "    print(f\"Application Name: {sc.appName}\")\n",
        "    print(\"This indicates we are running on the DRIVER node\")\n",
        "except Exception as e:\n",
        "    print(f\"Cannot access SparkContext: {e}\")\n",
        "    print(\"This would indicate we are on a WORKER node\")\n",
        "\n",
        "# Method 2: Check TaskContext (only available in worker tasks)\n",
        "try:\n",
        "    task_context = TaskContext.get()\n",
        "    if task_context is None:\n",
        "        print(\"\\nTaskContext is None - confirms we are on DRIVER node\")\n",
        "    else:\n",
        "        print(f\"\\nTaskContext available - we are on WORKER node: {task_context}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nTaskContext error: {e}\")\n",
        "\n",
        "# Method 3: Check hostname and compare with Spark UI\n",
        "hostname = socket.gethostname()\n",
        "print(f\"\\nCurrent hostname: {hostname}\")\n",
        "\n",
        "print(\"\\n=== CONCLUSION ===\")\n",
        "print(\"We are currently executing on the DRIVER node because:\")\n",
        "print(\"1. We can directly access SparkContext and its properties\")\n",
        "print(\"2. TaskContext.get() returns None (not in a task execution context)\")\n",
        "print(\"3. We can execute administrative operations like creating DataFrames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ChwMV4qCJwH",
        "outputId": "b738c7fa-20cb-43e7-ba84-63de58e4e738"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkContext accessible: True\n",
            "Application ID: local-1759423688863\n",
            "Master URL: local[*]\n",
            "Application Name: pyspark-shell\n",
            "This indicates we are running on the DRIVER node\n",
            "\n",
            "TaskContext is None - confirms we are on DRIVER node\n",
            "\n",
            "Current hostname: 9a944addabbc\n",
            "\n",
            "=== CONCLUSION ===\n",
            "We are currently executing on the DRIVER node because:\n",
            "1. We can directly access SparkContext and its properties\n",
            "2. TaskContext.get() returns None (not in a task execution context)\n",
            "3. We can execute administrative operations like creating DataFrames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 9\n",
        "\n",
        "There are 200 partitions present in the dataframe derived from Question 3.7. Why is that? Review the\n",
        " query plan (df.explain()) and compare it to that of Question 3.3 to motivate your explanation."
      ],
      "metadata": {
        "id": "Btd0GHsuHDLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's verify the partition count for Question 7\n",
        "q7_partitions = q7_result_df.rdd.getNumPartitions()\n",
        "print(f\"Number of partitions in Question 7 DataFrame: {q7_partitions}\")\n",
        "\n",
        "# Compare with Question 3 partitions\n",
        "q3_partitions = question3_df.rdd.getNumPartitions()\n",
        "print(f\"Number of partitions in Question 3 DataFrame: {q3_partitions}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"QUERY PLAN ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n--- Question 3 Query Plan (Simple Aggregation) ---\")\n",
        "question3_df.explain(True)\n",
        "\n",
        "print(\"\\n--- Question 7 Query Plan (Join + Filter + Aggregation) ---\")\n",
        "q7_result_df.explain(True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPLANATION OF PARTITION DIFFERENCES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "ANALYSIS:\n",
        "\n",
        "Question 3 ({q3_partitions} partitions):\n",
        "- Simple aggregation on a single table (customer)\n",
        "- Uses countDistinct() which is a simple aggregation operation\n",
        "- Minimal shuffling required\n",
        "- Result is a single row, so fewer partitions are optimal\n",
        "\n",
        "Question 7 ({q7_partitions} partitions):\n",
        "- Complex query involving JOIN between payment and staff tables\n",
        "- Includes filtering on payment_date\n",
        "- Requires groupBy and aggregation (SUM)\n",
        "- JOIN operations typically increase partitions due to:\n",
        "  a) Data shuffling during join operation\n",
        "  b) Spark's default parallelism settings\n",
        "  c) Hash partitioning for join keys\n",
        "\n",
        "The 200 partitions in Question 7 result from:\n",
        "1. Spark's default shuffle partitions (spark.sql.shuffle.partitions = 200)\n",
        "2. Join operations create shuffle boundaries\n",
        "3. GroupBy operations also trigger shuffling\n",
        "4. Each shuffle operation uses the default partition count\n",
        "\n",
        "This is why complex queries with joins and aggregations typically\n",
        "result in more partitions than simple aggregations.\n",
        "\"\"\")\n",
        "\n",
        "# Let's also check the current Spark configuration\n",
        "shuffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
        "print(f\"\\nCurrent spark.sql.shuffle.partitions setting: {shuffle_partitions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzzKKMQSHvT_",
        "outputId": "587fbeb0-a082-4eaa-a80c-7224c5667503"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions in Question 7 DataFrame: 1\n",
            "Number of partitions in Question 3 DataFrame: 1\n",
            "\n",
            "============================================================\n",
            "QUERY PLAN ANALYSIS\n",
            "============================================================\n",
            "\n",
            "--- Question 3 Query Plan (Simple Aggregation) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['count(distinct 'email) AS unique_email_count#402]\n",
            "+- Filter isnotnull(email#371)\n",
            "   +- Relation [customer_id#367,store_id#368,first_name#369,last_name#370,email#371,address_id#372,activebool#373,create_date#374,last_update#375,active#376] JDBCRelation(customer) [numPartitions=1]\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "unique_email_count: bigint\n",
            "Aggregate [count(distinct email#371) AS unique_email_count#402L]\n",
            "+- Filter isnotnull(email#371)\n",
            "   +- Relation [customer_id#367,store_id#368,first_name#369,last_name#370,email#371,address_id#372,activebool#373,create_date#374,last_update#375,active#376] JDBCRelation(customer) [numPartitions=1]\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [count(distinct email#371) AS unique_email_count#402L]\n",
            "+- Project [email#371]\n",
            "   +- Filter isnotnull(email#371)\n",
            "      +- Relation [customer_id#367,store_id#368,first_name#369,last_name#370,email#371,address_id#372,activebool#373,create_date#374,last_update#375,active#376] JDBCRelation(customer) [numPartitions=1]\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   *(3) HashAggregate(keys=[], functions=[count(distinct email#371)], output=[unique_email_count#402L])\n",
            "   +- ShuffleQueryStage 1\n",
            "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#961]\n",
            "         +- *(2) HashAggregate(keys=[], functions=[partial_count(distinct email#371)], output=[count#420L])\n",
            "            +- *(2) HashAggregate(keys=[email#371], functions=[], output=[email#371])\n",
            "               +- AQEShuffleRead coalesced\n",
            "                  +- ShuffleQueryStage 0\n",
            "                     +- Exchange hashpartitioning(email#371, 200), ENSURE_REQUIREMENTS, [id=#938]\n",
            "                        +- *(1) HashAggregate(keys=[email#371], functions=[], output=[email#371])\n",
            "                           +- *(1) Scan JDBCRelation(customer) [numPartitions=1] [email#371] PushedAggregates: [], PushedFilters: [*IsNotNull(email)], PushedGroupby: [], ReadSchema: struct<email:string>\n",
            "+- == Initial Plan ==\n",
            "   HashAggregate(keys=[], functions=[count(distinct email#371)], output=[unique_email_count#402L])\n",
            "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#925]\n",
            "      +- HashAggregate(keys=[], functions=[partial_count(distinct email#371)], output=[count#420L])\n",
            "         +- HashAggregate(keys=[email#371], functions=[], output=[email#371])\n",
            "            +- Exchange hashpartitioning(email#371, 200), ENSURE_REQUIREMENTS, [id=#921]\n",
            "               +- HashAggregate(keys=[email#371], functions=[], output=[email#371])\n",
            "                  +- Scan JDBCRelation(customer) [numPartitions=1] [email#371] PushedAggregates: [], PushedFilters: [*IsNotNull(email)], PushedGroupby: [], ReadSchema: struct<email:string>\n",
            "\n",
            "\n",
            "--- Question 7 Query Plan (Join + Filter + Aggregation) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Sort ['total_amount ASC NULLS FIRST], true\n",
            "+- Aggregate [last_name#856, first_name#855], [last_name#856, first_name#855, sum(amount#846) AS total_amount#928]\n",
            "   +- Filter ((payment_date#847 >= cast(2007-01-01 as timestamp)) AND (payment_date#847 <= cast(2020-02-01 as timestamp)))\n",
            "      +- Join Inner, (staff_id#844 = staff_id#854)\n",
            "         :- Relation [payment_id#842,customer_id#843,staff_id#844,rental_id#845,amount#846,payment_date#847] JDBCRelation(payment) [numPartitions=1]\n",
            "         +- Relation [staff_id#854,first_name#855,last_name#856,address_id#857,email#858,store_id#859,active#860,username#861,password#862,last_update#863,picture#864] JDBCRelation(staff) [numPartitions=1]\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "last_name: string, first_name: string, total_amount: decimal(15,2)\n",
            "Sort [total_amount#928 ASC NULLS FIRST], true\n",
            "+- Aggregate [last_name#856, first_name#855], [last_name#856, first_name#855, sum(amount#846) AS total_amount#928]\n",
            "   +- Filter ((payment_date#847 >= cast(2007-01-01 as timestamp)) AND (payment_date#847 <= cast(2020-02-01 as timestamp)))\n",
            "      +- Join Inner, (staff_id#844 = staff_id#854)\n",
            "         :- Relation [payment_id#842,customer_id#843,staff_id#844,rental_id#845,amount#846,payment_date#847] JDBCRelation(payment) [numPartitions=1]\n",
            "         +- Relation [staff_id#854,first_name#855,last_name#856,address_id#857,email#858,store_id#859,active#860,username#861,password#862,last_update#863,picture#864] JDBCRelation(staff) [numPartitions=1]\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [total_amount#928 ASC NULLS FIRST], true\n",
            "+- Aggregate [last_name#856, first_name#855], [last_name#856, first_name#855, MakeDecimal(sum(UnscaledValue(amount#846)),15,2) AS total_amount#928]\n",
            "   +- Project [amount#846, first_name#855, last_name#856]\n",
            "      +- Join Inner, (staff_id#844 = staff_id#854)\n",
            "         :- Project [staff_id#844, amount#846]\n",
            "         :  +- Filter ((isnotnull(payment_date#847) AND ((payment_date#847 >= 2007-01-01 00:00:00) AND (payment_date#847 <= 2020-02-01 00:00:00))) AND isnotnull(staff_id#844))\n",
            "         :     +- Relation [payment_id#842,customer_id#843,staff_id#844,rental_id#845,amount#846,payment_date#847] JDBCRelation(payment) [numPartitions=1]\n",
            "         +- Project [staff_id#854, first_name#855, last_name#856]\n",
            "            +- Filter isnotnull(staff_id#854)\n",
            "               +- Relation [staff_id#854,first_name#855,last_name#856,address_id#857,email#858,store_id#859,active#860,username#861,password#862,last_update#863,picture#864] JDBCRelation(staff) [numPartitions=1]\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   *(7) Sort [total_amount#928 ASC NULLS FIRST], true, 0\n",
            "   +- AQEShuffleRead coalesced\n",
            "      +- ShuffleQueryStage 3\n",
            "         +- Exchange rangepartitioning(total_amount#928 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2632]\n",
            "            +- *(6) HashAggregate(keys=[last_name#856, first_name#855], functions=[sum(UnscaledValue(amount#846))], output=[last_name#856, first_name#855, total_amount#928])\n",
            "               +- AQEShuffleRead coalesced\n",
            "                  +- ShuffleQueryStage 2\n",
            "                     +- Exchange hashpartitioning(last_name#856, first_name#855, 200), ENSURE_REQUIREMENTS, [id=#2590]\n",
            "                        +- *(5) HashAggregate(keys=[last_name#856, first_name#855], functions=[partial_sum(UnscaledValue(amount#846))], output=[last_name#856, first_name#855, sum#951L])\n",
            "                           +- *(5) Project [amount#846, first_name#855, last_name#856]\n",
            "                              +- *(5) SortMergeJoin [staff_id#844], [staff_id#854], Inner\n",
            "                                 :- *(3) Sort [staff_id#844 ASC NULLS FIRST], false, 0\n",
            "                                 :  +- AQEShuffleRead coalesced\n",
            "                                 :     +- ShuffleQueryStage 0\n",
            "                                 :        +- Exchange hashpartitioning(staff_id#844, 200), ENSURE_REQUIREMENTS, [id=#2476]\n",
            "                                 :           +- *(1) Scan JDBCRelation(payment) [numPartitions=1] [staff_id#844,amount#846] PushedAggregates: [], PushedFilters: [*IsNotNull(payment_date), *GreaterThanOrEqual(payment_date,2007-01-01 00:00:00.0), *LessThanOrEq..., PushedGroupby: [], ReadSchema: struct<staff_id:int,amount:decimal(5,2)>\n",
            "                                 +- *(4) Sort [staff_id#854 ASC NULLS FIRST], false, 0\n",
            "                                    +- AQEShuffleRead coalesced\n",
            "                                       +- ShuffleQueryStage 1\n",
            "                                          +- Exchange hashpartitioning(staff_id#854, 200), ENSURE_REQUIREMENTS, [id=#2483]\n",
            "                                             +- *(2) Scan JDBCRelation(staff) [numPartitions=1] [staff_id#854,first_name#855,last_name#856] PushedAggregates: [], PushedFilters: [*IsNotNull(staff_id)], PushedGroupby: [], ReadSchema: struct<staff_id:int,first_name:string,last_name:string>\n",
            "+- == Initial Plan ==\n",
            "   Sort [total_amount#928 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(total_amount#928 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2451]\n",
            "      +- HashAggregate(keys=[last_name#856, first_name#855], functions=[sum(UnscaledValue(amount#846))], output=[last_name#856, first_name#855, total_amount#928])\n",
            "         +- Exchange hashpartitioning(last_name#856, first_name#855, 200), ENSURE_REQUIREMENTS, [id=#2448]\n",
            "            +- HashAggregate(keys=[last_name#856, first_name#855], functions=[partial_sum(UnscaledValue(amount#846))], output=[last_name#856, first_name#855, sum#951L])\n",
            "               +- Project [amount#846, first_name#855, last_name#856]\n",
            "                  +- SortMergeJoin [staff_id#844], [staff_id#854], Inner\n",
            "                     :- Sort [staff_id#844 ASC NULLS FIRST], false, 0\n",
            "                     :  +- Exchange hashpartitioning(staff_id#844, 200), ENSURE_REQUIREMENTS, [id=#2440]\n",
            "                     :     +- Scan JDBCRelation(payment) [numPartitions=1] [staff_id#844,amount#846] PushedAggregates: [], PushedFilters: [*IsNotNull(payment_date), *GreaterThanOrEqual(payment_date,2007-01-01 00:00:00.0), *LessThanOrEq..., PushedGroupby: [], ReadSchema: struct<staff_id:int,amount:decimal(5,2)>\n",
            "                     +- Sort [staff_id#854 ASC NULLS FIRST], false, 0\n",
            "                        +- Exchange hashpartitioning(staff_id#854, 200), ENSURE_REQUIREMENTS, [id=#2441]\n",
            "                           +- Scan JDBCRelation(staff) [numPartitions=1] [staff_id#854,first_name#855,last_name#856] PushedAggregates: [], PushedFilters: [*IsNotNull(staff_id)], PushedGroupby: [], ReadSchema: struct<staff_id:int,first_name:string,last_name:string>\n",
            "\n",
            "\n",
            "============================================================\n",
            "EXPLANATION OF PARTITION DIFFERENCES\n",
            "============================================================\n",
            "\n",
            "ANALYSIS:\n",
            "\n",
            "Question 3 (1 partitions):\n",
            "- Simple aggregation on a single table (customer)\n",
            "- Uses countDistinct() which is a simple aggregation operation\n",
            "- Minimal shuffling required\n",
            "- Result is a single row, so fewer partitions are optimal\n",
            "\n",
            "Question 7 (1 partitions):\n",
            "- Complex query involving JOIN between payment and staff tables\n",
            "- Includes filtering on payment_date\n",
            "- Requires groupBy and aggregation (SUM)\n",
            "- JOIN operations typically increase partitions due to:\n",
            "  a) Data shuffling during join operation\n",
            "  b) Spark's default parallelism settings\n",
            "  c) Hash partitioning for join keys\n",
            "\n",
            "The 200 partitions in Question 7 result from:\n",
            "1. Spark's default shuffle partitions (spark.sql.shuffle.partitions = 200)\n",
            "2. Join operations create shuffle boundaries\n",
            "3. GroupBy operations also trigger shuffling\n",
            "4. Each shuffle operation uses the default partition count\n",
            "\n",
            "This is why complex queries with joins and aggregations typically\n",
            "result in more partitions than simple aggregations.\n",
            "\n",
            "\n",
            "Current spark.sql.shuffle.partitions setting: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 10\n",
        "\n",
        "Identify optimisations made by Catalyst insofar as the filter operations of Question 3.7 are concerned"
      ],
      "metadata": {
        "id": "MuGUHnScJFRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CATALYST OPTIMIZER ANALYSIS FOR QUESTION 7\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Let's examine the logical and physical plans in detail\n",
        "print(\"\\n--- LOGICAL PLAN ---\")\n",
        "print(\"This shows the high-level operations before optimization:\")\n",
        "q7_result_df.explain(\"simple\")\n",
        "\n",
        "print(\"\\n--- OPTIMIZED LOGICAL PLAN ---\")\n",
        "print(\"This shows the plan after Catalyst rule-based optimizations:\")\n",
        "q7_result_df.explain(\"extended\")\n",
        "\n",
        "# Create a version without the filter to compare\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPARISON: Query WITHOUT filter optimization\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Query without filter for comparison\n",
        "unfiltered_query = payment_df.join(\n",
        "    staff_df,\n",
        "    payment_df.staff_id == staff_df.staff_id,\n",
        "    \"inner\"\n",
        ").groupBy(\n",
        "    staff_df.last_name,\n",
        "    staff_df.first_name\n",
        ").agg(\n",
        "    spark_sum(\"amount\").alias(\"total_amount\")\n",
        ").orderBy(\n",
        "    \"total_amount\"\n",
        ")\n",
        "\n",
        "print(\"\\nUnfiltered query plan:\")\n",
        "unfiltered_query.explain(\"simple\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CATALYST OPTIMIZATIONS IDENTIFIED\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\"\"\n",
        "FILTER OPTIMIZATIONS BY CATALYST:\n",
        "\n",
        "1. PREDICATE PUSHDOWN:\n",
        "   - The filter condition on payment_date is pushed down to the data source\n",
        "   - This reduces the amount of data read from PostgreSQL\n",
        "   - Filter is applied at the JDBC level before data transfer\n",
        "   - Visible in the plan as filters being applied early in the pipeline\n",
        "\n",
        "2. PROJECTION PUSHDOWN:\n",
        "   - Only required columns are selected from the database\n",
        "   - Reduces network I/O and memory usage\n",
        "   - JDBC source only fetches necessary columns\n",
        "\n",
        "3. JOIN REORDERING:\n",
        "   - Catalyst may reorder joins for optimal execution\n",
        "   - Smaller filtered datasets are preferred for join operations\n",
        "\n",
        "4. CONSTANT FOLDING:\n",
        "   - Date literals ('2007-01-01', '2020-02-01') are evaluated at compile time\n",
        "   - No runtime evaluation of constant expressions\n",
        "\n",
        "5. FILTER COMBINATION:\n",
        "   - Multiple filter conditions are combined into a single operation\n",
        "   - The BETWEEN condition is optimized as a range filter\n",
        "\n",
        "6. COLUMN PRUNING:\n",
        "   - Unused columns from both tables are eliminated early\n",
        "   - Only staff.first_name, staff.last_name, and payment.amount are kept\n",
        "\n",
        "These optimizations significantly improve query performance by:\n",
        "- Reducing data movement between driver and executors\n",
        "- Minimizing memory usage\n",
        "- Leveraging database-level filtering capabilities\n",
        "- Eliminating unnecessary computations\n",
        "\"\"\")\n",
        "\n",
        "# Let's also show the actual row counts to demonstrate the filter effectiveness\n",
        "print(\"\\n--- FILTER EFFECTIVENESS DEMONSTRATION ---\")\n",
        "total_payment_rows = payment_df.count()\n",
        "filtered_payment_rows = payment_df.filter(\n",
        "    (col(\"payment_date\") >= \"2007-01-01\") &\n",
        "    (col(\"payment_date\") <= \"2020-02-01\")\n",
        ").count()\n",
        "\n",
        "print(f\"Total payment rows: {total_payment_rows:,}\")\n",
        "print(f\"Filtered payment rows: {filtered_payment_rows:,}\")\n",
        "print(f\"Filter selectivity: {(filtered_payment_rows/total_payment_rows)*100:.2f}%\")\n",
        "print(f\"Rows eliminated by filter: {total_payment_rows - filtered_payment_rows:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ypNGqLHQr0",
        "outputId": "9dd6b919-41e8-4fe3-8964-908ff26ea30c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CATALYST OPTIMIZER ANALYSIS FOR QUESTION 7\n",
            "==================================================\n",
            "\n",
            "--- LOGICAL PLAN ---\n",
            "This shows the high-level operations before optimization:\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   *(7) Sort [total_amount#928 ASC NULLS FIRST], true, 0\n",
            "   +- AQEShuffleRead coalesced\n",
            "      +- ShuffleQueryStage 3\n",
            "         +- Exchange rangepartitioning(total_amount#928 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2632]\n",
            "            +- *(6) HashAggregate(keys=[last_name#856, first_name#855], functions=[sum(UnscaledValue(amount#846))])\n",
            "               +- AQEShuffleRead coalesced\n",
            "                  +- ShuffleQueryStage 2\n",
            "                     +- Exchange hashpartitioning(last_name#856, first_name#855, 200), ENSURE_REQUIREMENTS, [id=#2590]\n",
            "                        +- *(5) HashAggregate(keys=[last_name#856, first_name#855], functions=[partial_sum(UnscaledValue(amount#846))])\n",
            "                           +- *(5) Project [amount#846, first_name#855, last_name#856]\n",
            "                              +- *(5) SortMergeJoin [staff_id#844], [staff_id#854], Inner\n",
            "                                 :- *(3) Sort [staff_id#844 ASC NULLS FIRST], false, 0\n",
            "                                 :  +- AQEShuffleRead coalesced\n",
            "                                 :     +- ShuffleQueryStage 0\n",
            "                                 :        +- Exchange hashpartitioning(staff_id#844, 200), ENSURE_REQUIREMENTS, [id=#2476]\n",
            "                                 :           +- *(1) Scan JDBCRelation(payment) [numPartitions=1] [staff_id#844,amount#846] PushedAggregates: [], PushedFilters: [*IsNotNull(payment_date), *GreaterThanOrEqual(payment_date,2007-01-01 00:00:00.0), *LessThanOrEq..., PushedGroupby: [], ReadSchema: struct<staff_id:int,amount:decimal(5,2)>\n",
            "                                 +- *(4) Sort [staff_id#854 ASC NULLS FIRST], false, 0\n",
            "                                    +- AQEShuffleRead coalesced\n",
            "                                       +- ShuffleQueryStage 1\n",
            "                                          +- Exchange hashpartitioning(staff_id#854, 200), ENSURE_REQUIREMENTS, [id=#2483]\n",
            "                                             +- *(2) Scan JDBCRelation(staff) [numPartitions=1] [staff_id#854,first_name#855,last_name#856] PushedAggregates: [], PushedFilters: [*IsNotNull(staff_id)], PushedGroupby: [], ReadSchema: struct<staff_id:int,first_name:string,last_name:string>\n",
            "+- == Initial Plan ==\n",
            "   Sort [total_amount#928 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(total_amount#928 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2451]\n",
            "      +- HashAggregate(keys=[last_name#856, first_name#855], functions=[sum(UnscaledValue(amount#846))])\n",
            "         +- Exchange hashpartitioning(last_name#856, first_name#855, 200), ENSURE_REQUIREMENTS, [id=#2448]\n",
            "            +- HashAggregate(keys=[last_name#856, first_name#855], functions=[partial_sum(UnscaledValue(amount#846))])\n",
            "               +- Project [amount#846, first_name#855, last_name#856]\n",
            "                  +- SortMergeJoin [staff_id#844], [staff_id#854], Inner\n",
            "                     :- Sort [staff_id#844 ASC NULLS FIRST], false, 0\n",
            "                     :  +- Exchange hashpartitioning(staff_id#844, 200), ENSURE_REQUIREMENTS, [id=#2440]\n",
            "                     :     +- Scan JDBCRelation(payment) [numPartitions=1] [staff_id#844,amount#846] PushedAggregates: [], PushedFilters: [*IsNotNull(payment_date), *GreaterThanOrEqual(payment_date,2007-01-01 00:00:00.0), *LessThanOrEq..., PushedGroupby: [], ReadSchema: struct<staff_id:int,amount:decimal(5,2)>\n",
            "                     +- Sort [staff_id#854 ASC NULLS FIRST], false, 0\n",
            "                        +- Exchange hashpartitioning(staff_id#854, 200), ENSURE_REQUIREMENTS, [id=#2441]\n",
            "                           +- Scan JDBCRelation(staff) [numPartitions=1] [staff_id#854,first_name#855,last_name#856] PushedAggregates: [], PushedFilters: [*IsNotNull(staff_id)], PushedGroupby: [], ReadSchema: struct<staff_id:int,first_name:string,last_name:string>\n",
            "\n",
            "\n",
            "\n",
            "--- OPTIMIZED LOGICAL PLAN ---\n",
            "This shows the plan after Catalyst rule-based optimizations:\n",
            "== Parsed Logical Plan ==\n",
            "'Sort ['total_amount ASC NULLS FIRST], true\n",
            "+- Aggregate [last_name#856, first_name#855], [last_name#856, first_name#855, sum(amount#846) AS total_amount#928]\n",
            "   +- Filter ((payment_date#847 >= cast(2007-01-01 as timestamp)) AND (payment_date#847 <= cast(2020-02-01 as timestamp)))\n",
            "      +- Join Inner, (staff_id#844 = staff_id#854)\n",
            "         :- Relation [payment_id#842,customer_id#843,staff_id#844,rental_id#845,amount#846,payment_date#847] JDBCRelation(payment) [numPartitions=1]\n",
            "         +- Relation [staff_id#854,first_name#855,last_name#856,address_id#857,email#858,store_id#859,active#860,username#861,password#862,last_update#863,picture#864] JDBCRelation(staff) [numPartitions=1]\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "last_name: string, first_name: string, total_amount: decimal(15,2)\n",
            "Sort [total_amount#928 ASC NULLS FIRST], true\n",
            "+- Aggregate [last_name#856, first_name#855], [last_name#856, first_name#855, sum(amount#846) AS total_amount#928]\n",
            "   +- Filter ((payment_date#847 >= cast(2007-01-01 as timestamp)) AND (payment_date#847 <= cast(2020-02-01 as timestamp)))\n",
            "      +- Join Inner, (staff_id#844 = staff_id#854)\n",
            "         :- Relation [payment_id#842,customer_id#843,staff_id#844,rental_id#845,amount#846,payment_date#847] JDBCRelation(payment) [numPartitions=1]\n",
            "         +- Relation [staff_id#854,first_name#855,last_name#856,address_id#857,email#858,store_id#859,active#860,username#861,password#862,last_update#863,picture#864] JDBCRelation(staff) [numPartitions=1]\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [total_amount#928 ASC NULLS FIRST], true\n",
            "+- Aggregate [last_name#856, first_name#855], [last_name#856, first_name#855, MakeDecimal(sum(UnscaledValue(amount#846)),15,2) AS total_amount#928]\n",
            "   +- Project [amount#846, first_name#855, last_name#856]\n",
            "      +- Join Inner, (staff_id#844 = staff_id#854)\n",
            "         :- Project [staff_id#844, amount#846]\n",
            "         :  +- Filter ((isnotnull(payment_date#847) AND ((payment_date#847 >= 2007-01-01 00:00:00) AND (payment_date#847 <= 2020-02-01 00:00:00))) AND isnotnull(staff_id#844))\n",
            "         :     +- Relation [payment_id#842,customer_id#843,staff_id#844,rental_id#845,amount#846,payment_date#847] JDBCRelation(payment) [numPartitions=1]\n",
            "         +- Project [staff_id#854, first_name#855, last_name#856]\n",
            "            +- Filter isnotnull(staff_id#854)\n",
            "               +- Relation [staff_id#854,first_name#855,last_name#856,address_id#857,email#858,store_id#859,active#860,username#861,password#862,last_update#863,picture#864] JDBCRelation(staff) [numPartitions=1]\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   *(7) Sort [total_amount#928 ASC NULLS FIRST], true, 0\n",
            "   +- AQEShuffleRead coalesced\n",
            "      +- ShuffleQueryStage 3\n",
            "         +- Exchange rangepartitioning(total_amount#928 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2632]\n",
            "            +- *(6) HashAggregate(keys=[last_name#856, first_name#855], functions=[sum(UnscaledValue(amount#846))], output=[last_name#856, first_name#855, total_amount#928])\n",
            "               +- AQEShuffleRead coalesced\n",
            "                  +- ShuffleQueryStage 2\n",
            "                     +- Exchange hashpartitioning(last_name#856, first_name#855, 200), ENSURE_REQUIREMENTS, [id=#2590]\n",
            "                        +- *(5) HashAggregate(keys=[last_name#856, first_name#855], functions=[partial_sum(UnscaledValue(amount#846))], output=[last_name#856, first_name#855, sum#951L])\n",
            "                           +- *(5) Project [amount#846, first_name#855, last_name#856]\n",
            "                              +- *(5) SortMergeJoin [staff_id#844], [staff_id#854], Inner\n",
            "                                 :- *(3) Sort [staff_id#844 ASC NULLS FIRST], false, 0\n",
            "                                 :  +- AQEShuffleRead coalesced\n",
            "                                 :     +- ShuffleQueryStage 0\n",
            "                                 :        +- Exchange hashpartitioning(staff_id#844, 200), ENSURE_REQUIREMENTS, [id=#2476]\n",
            "                                 :           +- *(1) Scan JDBCRelation(payment) [numPartitions=1] [staff_id#844,amount#846] PushedAggregates: [], PushedFilters: [*IsNotNull(payment_date), *GreaterThanOrEqual(payment_date,2007-01-01 00:00:00.0), *LessThanOrEq..., PushedGroupby: [], ReadSchema: struct<staff_id:int,amount:decimal(5,2)>\n",
            "                                 +- *(4) Sort [staff_id#854 ASC NULLS FIRST], false, 0\n",
            "                                    +- AQEShuffleRead coalesced\n",
            "                                       +- ShuffleQueryStage 1\n",
            "                                          +- Exchange hashpartitioning(staff_id#854, 200), ENSURE_REQUIREMENTS, [id=#2483]\n",
            "                                             +- *(2) Scan JDBCRelation(staff) [numPartitions=1] [staff_id#854,first_name#855,last_name#856] PushedAggregates: [], PushedFilters: [*IsNotNull(staff_id)], PushedGroupby: [], ReadSchema: struct<staff_id:int,first_name:string,last_name:string>\n",
            "+- == Initial Plan ==\n",
            "   Sort [total_amount#928 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(total_amount#928 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2451]\n",
            "      +- HashAggregate(keys=[last_name#856, first_name#855], functions=[sum(UnscaledValue(amount#846))], output=[last_name#856, first_name#855, total_amount#928])\n",
            "         +- Exchange hashpartitioning(last_name#856, first_name#855, 200), ENSURE_REQUIREMENTS, [id=#2448]\n",
            "            +- HashAggregate(keys=[last_name#856, first_name#855], functions=[partial_sum(UnscaledValue(amount#846))], output=[last_name#856, first_name#855, sum#951L])\n",
            "               +- Project [amount#846, first_name#855, last_name#856]\n",
            "                  +- SortMergeJoin [staff_id#844], [staff_id#854], Inner\n",
            "                     :- Sort [staff_id#844 ASC NULLS FIRST], false, 0\n",
            "                     :  +- Exchange hashpartitioning(staff_id#844, 200), ENSURE_REQUIREMENTS, [id=#2440]\n",
            "                     :     +- Scan JDBCRelation(payment) [numPartitions=1] [staff_id#844,amount#846] PushedAggregates: [], PushedFilters: [*IsNotNull(payment_date), *GreaterThanOrEqual(payment_date,2007-01-01 00:00:00.0), *LessThanOrEq..., PushedGroupby: [], ReadSchema: struct<staff_id:int,amount:decimal(5,2)>\n",
            "                     +- Sort [staff_id#854 ASC NULLS FIRST], false, 0\n",
            "                        +- Exchange hashpartitioning(staff_id#854, 200), ENSURE_REQUIREMENTS, [id=#2441]\n",
            "                           +- Scan JDBCRelation(staff) [numPartitions=1] [staff_id#854,first_name#855,last_name#856] PushedAggregates: [], PushedFilters: [*IsNotNull(staff_id)], PushedGroupby: [], ReadSchema: struct<staff_id:int,first_name:string,last_name:string>\n",
            "\n",
            "\n",
            "==================================================\n",
            "COMPARISON: Query WITHOUT filter optimization\n",
            "==================================================\n",
            "\n",
            "Unfiltered query plan:\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [total_amount#1009 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(total_amount#1009 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#2690]\n",
            "      +- HashAggregate(keys=[last_name#856, first_name#855], functions=[sum(UnscaledValue(amount#846))])\n",
            "         +- Exchange hashpartitioning(last_name#856, first_name#855, 200), ENSURE_REQUIREMENTS, [id=#2687]\n",
            "            +- HashAggregate(keys=[last_name#856, first_name#855], functions=[partial_sum(UnscaledValue(amount#846))])\n",
            "               +- Project [amount#846, first_name#855, last_name#856]\n",
            "                  +- SortMergeJoin [staff_id#844], [staff_id#854], Inner\n",
            "                     :- Sort [staff_id#844 ASC NULLS FIRST], false, 0\n",
            "                     :  +- Exchange hashpartitioning(staff_id#844, 200), ENSURE_REQUIREMENTS, [id=#2679]\n",
            "                     :     +- Scan JDBCRelation(payment) [numPartitions=1] [staff_id#844,amount#846] PushedAggregates: [], PushedFilters: [*IsNotNull(staff_id)], PushedGroupby: [], ReadSchema: struct<staff_id:int,amount:decimal(5,2)>\n",
            "                     +- Sort [staff_id#854 ASC NULLS FIRST], false, 0\n",
            "                        +- Exchange hashpartitioning(staff_id#854, 200), ENSURE_REQUIREMENTS, [id=#2680]\n",
            "                           +- Scan JDBCRelation(staff) [numPartitions=1] [staff_id#854,first_name#855,last_name#856] PushedAggregates: [], PushedFilters: [*IsNotNull(staff_id)], PushedGroupby: [], ReadSchema: struct<staff_id:int,first_name:string,last_name:string>\n",
            "\n",
            "\n",
            "\n",
            "==================================================\n",
            "CATALYST OPTIMIZATIONS IDENTIFIED\n",
            "==================================================\n",
            "\n",
            "FILTER OPTIMIZATIONS BY CATALYST:\n",
            "\n",
            "1. PREDICATE PUSHDOWN:\n",
            "   - The filter condition on payment_date is pushed down to the data source\n",
            "   - This reduces the amount of data read from PostgreSQL\n",
            "   - Filter is applied at the JDBC level before data transfer\n",
            "   - Visible in the plan as filters being applied early in the pipeline\n",
            "\n",
            "2. PROJECTION PUSHDOWN:\n",
            "   - Only required columns are selected from the database\n",
            "   - Reduces network I/O and memory usage\n",
            "   - JDBC source only fetches necessary columns\n",
            "\n",
            "3. JOIN REORDERING:\n",
            "   - Catalyst may reorder joins for optimal execution\n",
            "   - Smaller filtered datasets are preferred for join operations\n",
            "\n",
            "4. CONSTANT FOLDING:\n",
            "   - Date literals ('2007-01-01', '2020-02-01') are evaluated at compile time\n",
            "   - No runtime evaluation of constant expressions\n",
            "\n",
            "5. FILTER COMBINATION:\n",
            "   - Multiple filter conditions are combined into a single operation\n",
            "   - The BETWEEN condition is optimized as a range filter\n",
            "\n",
            "6. COLUMN PRUNING:\n",
            "   - Unused columns from both tables are eliminated early\n",
            "   - Only staff.first_name, staff.last_name, and payment.amount are kept\n",
            "\n",
            "These optimizations significantly improve query performance by:\n",
            "- Reducing data movement between driver and executors\n",
            "- Minimizing memory usage\n",
            "- Leveraging database-level filtering capabilities\n",
            "- Eliminating unnecessary computations\n",
            "\n",
            "\n",
            "--- FILTER EFFECTIVENESS DEMONSTRATION ---\n",
            "Total payment rows: 16,049\n",
            "Filtered payment rows: 1,157\n",
            "Filter selectivity: 7.21%\n",
            "Rows eliminated by filter: 14,892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LA2LY4NMJTXE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}