{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvpTHyheb49E"
      },
      "source": [
        "# Parallel Demand Forecasting with PySpark\n",
        "\n",
        "This notebook demonstrates how to implement fine-grained demand forecasting at the store-item level using PySpark for parallel processing. The implementation is based on the Databricks notebook but adapted for local machine/Google Colab execution.\n",
        "\n",
        "## Objectives\n",
        "1. Ingest data from remote CSV source\n",
        "2. Prepare and partition data for parallel processing\n",
        "3. Apply Prophet forecasting models to each store-item combination\n",
        "4. Persist forecasts and evaluate model performance\n",
        "5. Demonstrate parallel processing capabilities\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUEPA1BBb49H"
      },
      "source": [
        "## Question 1: Parallel Demand Forecasting Implementation\n",
        "\n",
        "### Setup and Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I_XTo66mb49I",
        "outputId": "d619f81c-a3b0-4287-ee8d-3b67e0c11668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.12/dist-packages (from prophet) (1.2.5)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.12/dist-packages (from prophet) (0.81)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.12/dist-packages (from prophet) (4.67.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from prophet) (6.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install pyspark prophet pandas numpy matplotlib seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ld9VHVO1b49J",
        "outputId": "f54e1bf3-c35f-4f85-8c3b-8a3e0599909b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "from typing import Iterator, Tuple\n",
        "import json\n",
        "\n",
        "# PySpark imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Prophet for forecasting\n",
        "from prophet import Prophet\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTlLXIqsb49J"
      },
      "source": [
        "### Initialize Spark Session with Optimized Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WeeNA0e7b49K",
        "outputId": "8a8a3d01-72f4-4f93-c907-9ed6ec150bb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session initialized successfully!\n",
            "Spark Version: 3.5.1\n",
            "Available cores: 2\n",
            "Master: local[*]\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Session with optimized configuration for local execution\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ParallelDemandForecasting\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbose output\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"Spark Session initialized successfully!\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTd3vZ71b49L"
      },
      "source": [
        "### Data Ingestion from Remote Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jFnqmqqFb49L",
        "outputId": "93eb23d5-17d1-4e73-e16f-9f5701287542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingesting data from remote source...\n",
            "Attempting to read data using pandas...\n",
            "Data ingestion completed in 34.17 seconds\n",
            "Total records loaded: 913,000\n",
            "\n",
            "Dataset Schema:\n",
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- store: long (nullable = true)\n",
            " |-- item: long (nullable = true)\n",
            " |-- sales: long (nullable = true)\n",
            "\n",
            "\n",
            "Sample Data:\n",
            "+----------+-----+----+-----+\n",
            "|      date|store|item|sales|\n",
            "+----------+-----+----+-----+\n",
            "|2013-01-01|    1|   1|   13|\n",
            "|2013-01-02|    1|   1|   11|\n",
            "|2013-01-03|    1|   1|   14|\n",
            "|2013-01-04|    1|   1|   13|\n",
            "|2013-01-05|    1|   1|   10|\n",
            "|2013-01-06|    1|   1|   12|\n",
            "|2013-01-07|    1|   1|   10|\n",
            "|2013-01-08|    1|   1|    9|\n",
            "|2013-01-09|    1|   1|   12|\n",
            "|2013-01-10|    1|   1|    9|\n",
            "+----------+-----+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data ingestion from remote CSV source\n",
        "import builtins  # Import builtins to access Python's built-in max function\n",
        "data_url = \"https://storage.googleapis.com/bdt-demand-forecast/sales-data.csv\"\n",
        "\n",
        "print(\"Ingesting data from remote source...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Read data using pandas first, then convert to Spark DataFrame\n",
        "    # This approach is more robust for remote URLs\n",
        "    print(\"Attempting to read data using pandas...\")\n",
        "    pandas_df = pd.read_csv(data_url)\n",
        "\n",
        "    # Convert pandas DataFrame to Spark DataFrame\n",
        "    df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "    # Cache the dataframe for better performance\n",
        "    df.cache()\n",
        "\n",
        "    # Force evaluation to ensure data is loaded\n",
        "    row_count = df.count()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Data ingestion completed in {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Total records loaded: {row_count:,}\")\n",
        "\n",
        "    # Display schema and sample data\n",
        "    print(\"\\nDataset Schema:\")\n",
        "    df.printSchema()\n",
        "\n",
        "    print(\"\\nSample Data:\")\n",
        "    df.show(10)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during data ingestion: {str(e)}\")\n",
        "    # Fallback: create sample data for demonstration\n",
        "    print(\"Creating sample data for demonstration...\")\n",
        "\n",
        "    # Generate sample data\n",
        "    dates = pd.date_range('2018-01-01', '2022-12-31', freq='D')\n",
        "    sample_data = []\n",
        "\n",
        "    for store in range(1, 11):  # 10 stores\n",
        "        for item in range(1, 51):  # 50 items\n",
        "            for date in dates:\n",
        "                # Generate realistic sales data with trend and seasonality\n",
        "                base_sales = 10 + (item % 10) * 2\n",
        "                trend = (date - dates[0]).days * 0.001\n",
        "                seasonal = 5 * np.sin(2 * np.pi * date.dayofyear / 365.25)\n",
        "                noise = np.random.normal(0, 2)\n",
        "                # Use Python's built-in max function explicitly to avoid conflict with PySpark's max\n",
        "                sales = builtins.max(0, base_sales + trend + seasonal + noise)\n",
        "\n",
        "                sample_data.append({\n",
        "                    'date': date.strftime('%Y-%m-%d'),\n",
        "                    'store': store,\n",
        "                    'item': item,\n",
        "                    'sales': round(sales, 2)\n",
        "                })\n",
        "\n",
        "    # Convert to Spark DataFrame\n",
        "    df = spark.createDataFrame(sample_data)\n",
        "    df.cache()\n",
        "\n",
        "    print(f\"Sample data created with {df.count():,} records\")\n",
        "    df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIE_SJ0ib49M"
      },
      "source": [
        "### Data Preparation and Partitioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tIHAZ6ltb49N",
        "outputId": "deddeeed-192d-441d-93f2-6138f9a15078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data for parallel processing...\n",
            "Total store-item combinations: 500\n",
            "Data repartitioned into 8 partitions\n",
            "Optimal partitions calculated: 8\n",
            "\n",
            "Data Statistics:\n",
            "+-------+-----------------+------------------+----------+------------------+------------------+-----------------+------------------+\n",
            "|summary|            store|              item|store_item|             sales|              year|            month|       day_of_week|\n",
            "+-------+-----------------+------------------+----------+------------------+------------------+-----------------+------------------+\n",
            "|  count|           913000|            913000|    913000|            913000|            913000|           913000|            913000|\n",
            "|   mean|              5.5|              25.5|      NULL|52.250286966046005| 2015.000547645126| 6.52354874041621| 4.001095290251917|\n",
            "| stddev|2.872282896261173|14.430877592663762|      NULL|28.801143603517094|1.4140205956566094|3.448535031041885|2.0000007953757732|\n",
            "|    min|                1|                 1|      10_1|                 0|              2013|                1|                 1|\n",
            "|    max|               10|                50|       9_9|               231|              2017|               12|                 7|\n",
            "+-------+-----------------+------------------+----------+------------------+------------------+-----------------+------------------+\n",
            "\n",
            "\n",
            "Prepared Data Sample:\n",
            "+----------+-----+----+----------+-----+----+-----+-----------+\n",
            "|      date|store|item|store_item|sales|year|month|day_of_week|\n",
            "+----------+-----+----+----------+-----+----+-----+-----------+\n",
            "|2013-01-01|    1|   1|       1_1|   13|2013|    1|          3|\n",
            "|2013-01-02|    1|   1|       1_1|   11|2013|    1|          4|\n",
            "|2013-01-03|    1|   1|       1_1|   14|2013|    1|          5|\n",
            "|2013-01-04|    1|   1|       1_1|   13|2013|    1|          6|\n",
            "|2013-01-05|    1|   1|       1_1|   10|2013|    1|          7|\n",
            "|2013-01-06|    1|   1|       1_1|   12|2013|    1|          1|\n",
            "|2013-01-07|    1|   1|       1_1|   10|2013|    1|          2|\n",
            "|2013-01-08|    1|   1|       1_1|    9|2013|    1|          3|\n",
            "|2013-01-09|    1|   1|       1_1|   12|2013|    1|          4|\n",
            "|2013-01-10|    1|   1|       1_1|    9|2013|    1|          5|\n",
            "+----------+-----+----+----------+-----+----+-----+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data preparation and partitioning\n",
        "import builtins  # Import builtins to access Python's built-in functions\n",
        "print(\"Preparing data for parallel processing...\")\n",
        "\n",
        "# Convert date column to proper date type and add derived features\n",
        "store_item_history = df \\\n",
        "    .withColumn(\"date\", to_date(col(\"date\"))) \\\n",
        "    .withColumn(\"store_item\", concat(col(\"store\"), lit(\"_\"), col(\"item\"))) \\\n",
        "    .withColumn(\"year\", year(col(\"date\"))) \\\n",
        "    .withColumn(\"month\", month(col(\"date\"))) \\\n",
        "    .withColumn(\"day_of_week\", dayofweek(col(\"date\"))) \\\n",
        "    .select(\"date\", \"store\", \"item\", \"store_item\", \"sales\", \"year\", \"month\", \"day_of_week\")\n",
        "\n",
        "# Get unique store-item combinations\n",
        "store_item_combinations = store_item_history.select(\"store_item\").distinct().count()\n",
        "print(f\"Total store-item combinations: {store_item_combinations}\")\n",
        "\n",
        "# Repartition data by store_item for optimal parallel processing\n",
        "# Use a number of partitions that balances parallelism with overhead\n",
        "# Use Python's built-in min function explicitly to avoid conflict with PySpark's min\n",
        "optimal_partitions = builtins.min(store_item_combinations, spark.sparkContext.defaultParallelism * 4)\n",
        "\n",
        "store_item_history = store_item_history \\\n",
        "    .repartition(optimal_partitions, \"store_item\")\n",
        "\n",
        "# Cache the repartitioned data\n",
        "store_item_history.cache()\n",
        "\n",
        "# Force evaluation to ensure repartitioning is complete\n",
        "_ = store_item_history.count()\n",
        "\n",
        "print(f\"Data repartitioned into {store_item_history.rdd.getNumPartitions()} partitions\")\n",
        "print(f\"Optimal partitions calculated: {optimal_partitions}\")\n",
        "\n",
        "# Display data statistics\n",
        "print(\"\\nData Statistics:\")\n",
        "store_item_history.describe().show()\n",
        "\n",
        "# Show sample of prepared data\n",
        "print(\"\\nPrepared Data Sample:\")\n",
        "store_item_history.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wOCzFngb49N"
      },
      "source": [
        "### Define Forecasting and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GAogZHuDb49O",
        "outputId": "e3d87c60-9289-4c6a-c9e1-2836c6737c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forecasting and evaluation functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define the forecast function for individual store-item combinations\n",
        "def forecast_store_item(store_item_data):\n",
        "    \"\"\"\n",
        "    Forecast sales for a single store-item combination using Prophet.\n",
        "\n",
        "    Args:\n",
        "        store_item_data: Pandas DataFrame with columns ['date', 'sales']\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing forecast results and metadata\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
        "        prophet_data = store_item_data[['date', 'sales']].copy()\n",
        "        prophet_data.columns = ['ds', 'y']\n",
        "        prophet_data = prophet_data.sort_values('ds')\n",
        "\n",
        "        # Remove any missing values\n",
        "        prophet_data = prophet_data.dropna()\n",
        "\n",
        "        if len(prophet_data) < 30:  # Need sufficient data for forecasting\n",
        "            return {\n",
        "                'store_item': store_item_data['store_item'].iloc[0],\n",
        "                'status': 'insufficient_data',\n",
        "                'forecast': None,\n",
        "                'error': 'Insufficient data points for forecasting'\n",
        "            }\n",
        "\n",
        "        # Initialize and fit Prophet model\n",
        "        model = Prophet(\n",
        "            daily_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            yearly_seasonality=True,\n",
        "            seasonality_mode='multiplicative',\n",
        "            interval_width=0.95\n",
        "        )\n",
        "\n",
        "        # Suppress Prophet's verbose output\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            model.fit(prophet_data)\n",
        "\n",
        "        # Create future dataframe for forecasting (30 days ahead)\n",
        "        future = model.make_future_dataframe(periods=30)\n",
        "\n",
        "        # Generate forecast\n",
        "        forecast = model.predict(future)\n",
        "\n",
        "        # Extract relevant forecast information\n",
        "        forecast_result = {\n",
        "            'store_item': store_item_data['store_item'].iloc[0],\n",
        "            'store': store_item_data['store'].iloc[0],\n",
        "            'item': store_item_data['item'].iloc[0],\n",
        "            'status': 'success',\n",
        "            'forecast_dates': forecast['ds'].tail(30).tolist(),\n",
        "            'forecast_values': forecast['yhat'].tail(30).tolist(),\n",
        "            'forecast_lower': forecast['yhat_lower'].tail(30).tolist(),\n",
        "            'forecast_upper': forecast['yhat_upper'].tail(30).tolist(),\n",
        "            'historical_dates': prophet_data['ds'].tolist(),\n",
        "            'historical_values': prophet_data['y'].tolist(),\n",
        "            'fitted_values': forecast['yhat'].iloc[:-30].tolist(),\n",
        "            'training_period': f\"{prophet_data['ds'].min()} to {prophet_data['ds'].max()}\",\n",
        "            'forecast_period': f\"{forecast['ds'].tail(30).min()} to {forecast['ds'].tail(30).max()}\"\n",
        "        }\n",
        "\n",
        "        return forecast_result\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'store_item': store_item_data['store_item'].iloc[0] if 'store_item' in store_item_data.columns else 'unknown',\n",
        "            'status': 'error',\n",
        "            'forecast': None,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate_forecast(forecast_result):\n",
        "    \"\"\"\n",
        "    Evaluate forecast performance using multiple metrics.\n",
        "\n",
        "    Args:\n",
        "        forecast_result: Dictionary containing forecast results\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if forecast_result['status'] != 'success':\n",
        "            return {\n",
        "                'store_item': forecast_result['store_item'],\n",
        "                'status': 'evaluation_failed',\n",
        "                'error': f\"Cannot evaluate: {forecast_result.get('error', 'Unknown error')}\"\n",
        "            }\n",
        "\n",
        "        # Get historical and fitted values for evaluation\n",
        "        actual = np.array(forecast_result['historical_values'])\n",
        "        fitted = np.array(forecast_result['fitted_values'])\n",
        "\n",
        "        # Ensure arrays have the same length\n",
        "        min_length = min(len(actual), len(fitted))\n",
        "        actual = actual[:min_length]\n",
        "        fitted = fitted[:min_length]\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        mae = np.mean(np.abs(actual - fitted))\n",
        "        mse = np.mean((actual - fitted) ** 2)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        # MAPE (Mean Absolute Percentage Error)\n",
        "        # Avoid division by zero\n",
        "        non_zero_actual = actual[actual != 0]\n",
        "        non_zero_fitted = fitted[actual != 0]\n",
        "\n",
        "        if len(non_zero_actual) > 0:\n",
        "            mape = np.mean(np.abs((non_zero_actual - non_zero_fitted) / non_zero_actual)) * 100\n",
        "        else:\n",
        "            mape = np.inf\n",
        "\n",
        "        # R-squared\n",
        "        ss_res = np.sum((actual - fitted) ** 2)\n",
        "        ss_tot = np.sum((actual - np.mean(actual)) ** 2)\n",
        "        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
        "\n",
        "        # Additional metrics\n",
        "        mean_actual = np.mean(actual)\n",
        "        mean_forecast = np.mean(fitted)\n",
        "        bias = mean_forecast - mean_actual\n",
        "\n",
        "        evaluation_result = {\n",
        "            'store_item': forecast_result['store_item'],\n",
        "            'store': forecast_result['store'],\n",
        "            'item': forecast_result['item'],\n",
        "            'status': 'success',\n",
        "            'mae': round(mae, 4),\n",
        "            'mse': round(mse, 4),\n",
        "            'rmse': round(rmse, 4),\n",
        "            'mape': round(mape, 4) if mape != np.inf else 'inf',\n",
        "            'r_squared': round(r_squared, 4),\n",
        "            'bias': round(bias, 4),\n",
        "            'mean_actual': round(mean_actual, 4),\n",
        "            'mean_forecast': round(mean_forecast, 4),\n",
        "            'data_points': len(actual),\n",
        "            'training_period': forecast_result['training_period']\n",
        "        }\n",
        "\n",
        "        return evaluation_result\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'store_item': forecast_result.get('store_item', 'unknown'),\n",
        "            'status': 'evaluation_error',\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "print(\"Forecasting and evaluation functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFFOqXLUb49P"
      },
      "source": [
        "### Parallel Model Fitting and Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1fJqfFHdb49P",
        "outputId": "703759cf-025c-4083-cfc4-3399eeff8efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting parallel forecasting process...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 33.0 failed 1 times, most recent failure: Lost task 1.0 in stage 33.0 (TID 44) (549be5b0325c executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/tmp/ipython-input-4102893185.py\", line 22, in process_partition\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 9183, in groupby\n    return DataFrameGroupBy(\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\", line 1329, in __init__\n    grouper, exclusions, obj = get_grouper(\n                               ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/grouper.py\", line 1043, in get_grouper\n    raise KeyError(gpr)\nKeyError: 'store_item'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/tmp/ipython-input-4102893185.py\", line 22, in process_partition\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 9183, in groupby\n    return DataFrameGroupBy(\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\", line 1329, in __init__\n    grouper, exclusions, obj = get_grouper(\n                               ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/grouper.py\", line 1043, in get_grouper\n    raise KeyError(gpr)\nKeyError: 'store_item'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4102893185.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Collect results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mall_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecast_results_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 33.0 failed 1 times, most recent failure: Lost task 1.0 in stage 33.0 (TID 44) (549be5b0325c executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/tmp/ipython-input-4102893185.py\", line 22, in process_partition\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 9183, in groupby\n    return DataFrameGroupBy(\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\", line 1329, in __init__\n    grouper, exclusions, obj = get_grouper(\n                               ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/grouper.py\", line 1043, in get_grouper\n    raise KeyError(gpr)\nKeyError: 'store_item'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/tmp/ipython-input-4102893185.py\", line 22, in process_partition\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\", line 9183, in groupby\n    return DataFrameGroupBy(\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\", line 1329, in __init__\n    grouper, exclusions, obj = get_grouper(\n                               ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/grouper.py\", line 1043, in get_grouper\n    raise KeyError(gpr)\nKeyError: 'store_item'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ],
      "source": [
        "# Parallel model fitting and forecasting using PySpark\n",
        "print(\"Starting parallel forecasting process...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Function to process each partition\n",
        "def process_partition(partition_data):\n",
        "    \"\"\"\n",
        "    Process a partition of data containing multiple store-item combinations.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Convert iterator to list to work with pandas\n",
        "    partition_list = list(partition_data)\n",
        "\n",
        "    if not partition_list:\n",
        "        return iter(results)\n",
        "\n",
        "    # Convert to pandas DataFrame\n",
        "    partition_df = pd.DataFrame(partition_list)\n",
        "\n",
        "    # Group by store_item and process each group\n",
        "    for store_item, group_data in partition_df.groupby('store_item'):\n",
        "        # Forecast for this store-item combination\n",
        "        forecast_result = forecast_store_item(group_data)\n",
        "\n",
        "        # Evaluate the forecast\n",
        "        evaluation_result = evaluate_forecast(forecast_result)\n",
        "\n",
        "        # Combine results\n",
        "        combined_result = {\n",
        "            'forecast': forecast_result,\n",
        "            'evaluation': evaluation_result\n",
        "        }\n",
        "\n",
        "        results.append(combined_result)\n",
        "\n",
        "    return iter(results)\n",
        "\n",
        "# Apply the forecasting function to each partition\n",
        "forecast_results_rdd = store_item_history.rdd.mapPartitions(process_partition)\n",
        "\n",
        "# Collect results\n",
        "all_results = forecast_results_rdd.collect()\n",
        "\n",
        "end_time = time.time()\n",
        "processing_time = end_time - start_time\n",
        "\n",
        "print(f\"Parallel forecasting completed in {processing_time:.2f} seconds\")\n",
        "print(f\"Processed {len(all_results)} store-item combinations\")\n",
        "\n",
        "# Separate forecast and evaluation results\n",
        "forecast_results = [result['forecast'] for result in all_results]\n",
        "evaluation_results = [result['evaluation'] for result in all_results]\n",
        "\n",
        "# Count successful vs failed forecasts\n",
        "successful_forecasts = sum(1 for r in forecast_results if r['status'] == 'success')\n",
        "failed_forecasts = len(forecast_results) - successful_forecasts\n",
        "\n",
        "print(f\"Successful forecasts: {successful_forecasts}\")\n",
        "print(f\"Failed forecasts: {failed_forecasts}\")\n",
        "\n",
        "if failed_forecasts > 0:\n",
        "    print(\"\\nFailure reasons:\")\n",
        "    failure_reasons = {}\n",
        "    for r in forecast_results:\n",
        "        if r['status'] != 'success':\n",
        "            reason = r.get('error', 'Unknown error')\n",
        "            failure_reasons[reason] = failure_reasons.get(reason, 0) + 1\n",
        "\n",
        "    for reason, count in failure_reasons.items():\n",
        "        print(f\"  {reason}: {count} cases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTunDI38b49Q"
      },
      "source": [
        "### Persist Forecasts to Local Filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS_aXYoHb49Q"
      },
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = \"forecast_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Persisting forecast results to {output_dir}/...\")\n",
        "\n",
        "# Save forecast results as JSON\n",
        "forecast_file = os.path.join(output_dir, \"forecast_results.json\")\n",
        "with open(forecast_file, 'w') as f:\n",
        "    json.dump(forecast_results, f, indent=2, default=str)\n",
        "\n",
        "# Save evaluation results as JSON\n",
        "evaluation_file = os.path.join(output_dir, \"evaluation_results.json\")\n",
        "with open(evaluation_file, 'w') as f:\n",
        "    json.dump(evaluation_results, f, indent=2, default=str)\n",
        "\n",
        "# Create a summary CSV for easy analysis\n",
        "successful_evaluations = [r for r in evaluation_results if r['status'] == 'success']\n",
        "\n",
        "if successful_evaluations:\n",
        "    evaluation_df = pd.DataFrame(successful_evaluations)\n",
        "    summary_file = os.path.join(output_dir, \"evaluation_summary.csv\")\n",
        "    evaluation_df.to_csv(summary_file, index=False)\n",
        "\n",
        "    print(f\"Saved {len(forecast_results)} forecast results to {forecast_file}\")\n",
        "    print(f\"Saved {len(evaluation_results)} evaluation results to {evaluation_file}\")\n",
        "    print(f\"Saved evaluation summary to {summary_file}\")\n",
        "\n",
        "    # Display summary statistics\n",
        "    print(\"\\nEvaluation Summary Statistics:\")\n",
        "    print(evaluation_df[['mae', 'rmse', 'mape', 'r_squared']].describe())\n",
        "else:\n",
        "    print(\"No successful evaluations to save.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZcPmFSjb49Q"
      },
      "source": [
        "### Display Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5e4NKTmb49R"
      },
      "outputs": [],
      "source": [
        "# Display detailed evaluation results\n",
        "print(\"=\" * 80)\n",
        "print(\"FORECAST EVALUATION RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if successful_evaluations:\n",
        "    # Overall statistics\n",
        "    print(f\"\\nTotal Store-Item Combinations Processed: {len(evaluation_results)}\")\n",
        "    print(f\"Successful Evaluations: {len(successful_evaluations)}\")\n",
        "    print(f\"Success Rate: {len(successful_evaluations)/len(evaluation_results)*100:.1f}%\")\n",
        "\n",
        "    # Performance metrics summary\n",
        "    metrics_df = pd.DataFrame(successful_evaluations)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate aggregate metrics\n",
        "    avg_mae = metrics_df['mae'].mean()\n",
        "    avg_rmse = metrics_df['rmse'].mean()\n",
        "    avg_mape = metrics_df[metrics_df['mape'] != 'inf']['mape'].astype(float).mean()\n",
        "    avg_r_squared = metrics_df['r_squared'].mean()\n",
        "\n",
        "    print(f\"Average MAE (Mean Absolute Error): {avg_mae:.4f}\")\n",
        "    print(f\"Average RMSE (Root Mean Square Error): {avg_rmse:.4f}\")\n",
        "    print(f\"Average MAPE (Mean Absolute Percentage Error): {avg_mape:.2f}%\")\n",
        "    print(f\"Average R-squared: {avg_r_squared:.4f}\")\n",
        "\n",
        "    # Best and worst performing models\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"TOP 5 BEST PERFORMING MODELS (by R-squared)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    best_models = metrics_df.nlargest(5, 'r_squared')[['store_item', 'mae', 'rmse', 'mape', 'r_squared']]\n",
        "    for idx, row in best_models.iterrows():\n",
        "        print(f\"Store-Item: {row['store_item']} | MAE: {row['mae']:.4f} | RMSE: {row['rmse']:.4f} | MAPE: {row['mape']}% | R²: {row['r_squared']:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"TOP 5 WORST PERFORMING MODELS (by R-squared)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    worst_models = metrics_df.nsmallest(5, 'r_squared')[['store_item', 'mae', 'rmse', 'mape', 'r_squared']]\n",
        "    for idx, row in worst_models.iterrows():\n",
        "        print(f\"Store-Item: {row['store_item']} | MAE: {row['mae']:.4f} | RMSE: {row['rmse']:.4f} | MAPE: {row['mape']}% | R²: {row['r_squared']:.4f}\")\n",
        "\n",
        "    # Distribution of performance metrics\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"PERFORMANCE DISTRIBUTION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # R-squared distribution\n",
        "    excellent = (metrics_df['r_squared'] >= 0.8).sum()\n",
        "    good = ((metrics_df['r_squared'] >= 0.6) & (metrics_df['r_squared'] < 0.8)).sum()\n",
        "    fair = ((metrics_df['r_squared'] >= 0.4) & (metrics_df['r_squared'] < 0.6)).sum()\n",
        "    poor = (metrics_df['r_squared'] < 0.4).sum()\n",
        "\n",
        "    print(f\"Excellent models (R² ≥ 0.8): {excellent} ({excellent/len(metrics_df)*100:.1f}%)\")\n",
        "    print(f\"Good models (0.6 ≤ R² < 0.8): {good} ({good/len(metrics_df)*100:.1f}%)\")\n",
        "    print(f\"Fair models (0.4 ≤ R² < 0.6): {fair} ({fair/len(metrics_df)*100:.1f}%)\")\n",
        "    print(f\"Poor models (R² < 0.4): {poor} ({poor/len(metrics_df)*100:.1f}%)\")\n",
        "\n",
        "    # Processing performance\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"PROCESSING PERFORMANCE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(f\"Total Processing Time: {processing_time:.2f} seconds\")\n",
        "    print(f\"Average Time per Model: {processing_time/len(evaluation_results):.3f} seconds\")\n",
        "    print(f\"Models per Second: {len(evaluation_results)/processing_time:.2f}\")\n",
        "\n",
        "else:\n",
        "    print(\"No successful evaluations to display.\")\n",
        "\n",
        "    # Show error summary\n",
        "    error_summary = {}\n",
        "    for result in evaluation_results:\n",
        "        if result['status'] != 'success':\n",
        "            error = result.get('error', 'Unknown error')\n",
        "            error_summary[error] = error_summary.get(error, 0) + 1\n",
        "\n",
        "    print(\"\\nError Summary:\")\n",
        "    for error, count in error_summary.items():\n",
        "        print(f\"  {error}: {count} cases\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmT63kf5b49R"
      },
      "source": [
        "---\n",
        "\n",
        "## Question 2: Number of Partitions in store_item_history DataFrame\n",
        "\n",
        "Let's examine the partitioning details of our dataframe after repartitioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hOXcSReb49S"
      },
      "outputs": [],
      "source": [
        "# Question 2: Display partition information\n",
        "print(\"=\" * 60)\n",
        "print(\"PARTITION ANALYSIS - store_item_history DataFrame\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get number of partitions\n",
        "num_partitions = store_item_history.rdd.getNumPartitions()\n",
        "print(f\"\\nNumber of partitions after repartitioning: {num_partitions}\")\n",
        "\n",
        "# Get partition sizes\n",
        "partition_sizes = store_item_history.rdd.mapPartitions(lambda x: [sum(1 for _ in x)]).collect()\n",
        "print(f\"\\nPartition sizes: {partition_sizes}\")\n",
        "print(f\"Total records across all partitions: {sum(partition_sizes):,}\")\n",
        "print(f\"Average records per partition: {sum(partition_sizes)/len(partition_sizes):.1f}\")\n",
        "print(f\"Min records in a partition: {min(partition_sizes)}\")\n",
        "print(f\"Max records in a partition: {max(partition_sizes)}\")\n",
        "\n",
        "# Show partitioning strategy details\n",
        "print(f\"\\nPartitioning Strategy Details:\")\n",
        "print(f\"- Partitioned by: store_item column\")\n",
        "print(f\"- Total store-item combinations: {store_item_combinations}\")\n",
        "print(f\"- Available CPU cores: {spark.sparkContext.defaultParallelism}\")\n",
        "print(f\"- Optimal partitions calculated: {optimal_partitions}\")\n",
        "print(f\"- Actual partitions used: {num_partitions}\")\n",
        "\n",
        "# Visualize partition distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(partition_sizes)), partition_sizes)\n",
        "plt.title('Records per Partition')\n",
        "plt.xlabel('Partition Index')\n",
        "plt.ylabel('Number of Records')\n",
        "plt.xticks(range(0, len(partition_sizes), max(1, len(partition_sizes)//10)))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(partition_sizes, bins=min(20, len(partition_sizes)), alpha=0.7, edgecolor='black')\n",
        "plt.title('Distribution of Partition Sizes')\n",
        "plt.xlabel('Number of Records')\n",
        "plt.ylabel('Number of Partitions')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📸 Screenshot this cell output for Question 2 submission!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9WSPjWQb49S"
      },
      "source": [
        "---\n",
        "\n",
        "## Question 3: Demonstrating Parallel Processing Utilization\n",
        "\n",
        "Let's demonstrate that our process is utilizing the underlying compute resources in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyJc3ZJlb49S"
      },
      "outputs": [],
      "source": [
        "# Question 3: Demonstrate parallel processing utilization\n",
        "print(\"=\" * 70)\n",
        "print(\"PARALLEL PROCESSING DEMONSTRATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# System information\n",
        "import psutil\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "print(f\"\\nSystem Information:\")\n",
        "print(f\"- CPU cores (physical): {psutil.cpu_count(logical=False)}\")\n",
        "print(f\"- CPU cores (logical): {psutil.cpu_count(logical=True)}\")\n",
        "print(f\"- Available memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
        "print(f\"- Spark default parallelism: {spark.sparkContext.defaultParallelism}\")\n",
        "print(f\"- Spark master: {spark.sparkContext.master}\")\n",
        "\n",
        "# Monitor CPU usage during processing\n",
        "cpu_usage_data = []\n",
        "monitoring_active = True\n",
        "\n",
        "def monitor_cpu():\n",
        "    \"\"\"Monitor CPU usage in a separate thread\"\"\"\n",
        "    while monitoring_active:\n",
        "        cpu_percent = psutil.cpu_percent(interval=0.5, percpu=True)\n",
        "        cpu_usage_data.append({\n",
        "            'timestamp': time.time(),\n",
        "            'cpu_usage': cpu_percent,\n",
        "            'avg_cpu': sum(cpu_percent) / len(cpu_percent)\n",
        "        })\n",
        "\n",
        "# Start CPU monitoring\n",
        "monitor_thread = threading.Thread(target=monitor_cpu)\n",
        "monitor_thread.start()\n",
        "\n",
        "print(f\"\\nStarting parallel processing demonstration...\")\n",
        "demo_start_time = time.time()\n",
        "\n",
        "# Create a smaller subset for demonstration\n",
        "demo_data = store_item_history.sample(fraction=0.1, seed=42)  # 10% sample\n",
        "demo_data = demo_data.repartition(spark.sparkContext.defaultParallelism)\n",
        "demo_data.cache()\n",
        "\n",
        "# Force evaluation\n",
        "demo_count = demo_data.count()\n",
        "print(f\"Demo dataset size: {demo_count:,} records\")\n",
        "print(f\"Demo partitions: {demo_data.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Parallel processing with timing\n",
        "def simple_aggregation_task(partition_data):\n",
        "    \"\"\"Simple aggregation task to demonstrate parallel processing\"\"\"\n",
        "    partition_list = list(partition_data)\n",
        "    if not partition_list:\n",
        "        return iter([])\n",
        "\n",
        "    # Simulate some computation\n",
        "    import time\n",
        "    time.sleep(0.1)  # Small delay to make processing visible\n",
        "\n",
        "    df = pd.DataFrame(partition_list)\n",
        "    if 'store_item' in df.columns:\n",
        "        results = []\n",
        "        for store_item, group in df.groupby('store_item'):\n",
        "            result = {\n",
        "                'store_item': store_item,\n",
        "                'total_sales': group['sales'].sum(),\n",
        "                'avg_sales': group['sales'].mean(),\n",
        "                'record_count': len(group),\n",
        "                'processing_thread': threading.current_thread().name\n",
        "            }\n",
        "            results.append(result)\n",
        "        return iter(results)\n",
        "    return iter([])\n",
        "\n",
        "# Execute parallel processing\n",
        "parallel_start = time.time()\n",
        "parallel_results = demo_data.rdd.mapPartitions(simple_aggregation_task).collect()\n",
        "parallel_end = time.time()\n",
        "\n",
        "# Stop monitoring\n",
        "monitoring_active = False\n",
        "monitor_thread.join()\n",
        "\n",
        "demo_end_time = time.time()\n",
        "\n",
        "print(f\"\\nParallel Processing Results:\")\n",
        "print(f\"- Processed {len(parallel_results)} store-item combinations\")\n",
        "print(f\"- Processing time: {parallel_end - parallel_start:.2f} seconds\")\n",
        "print(f\"- Total demo time: {demo_end_time - demo_start_time:.2f} seconds\")\n",
        "\n",
        "# Analyze CPU usage during processing\n",
        "if cpu_usage_data:\n",
        "    print(f\"\\nCPU Utilization Analysis:\")\n",
        "    avg_cpu_usage = [data['avg_cpu'] for data in cpu_usage_data]\n",
        "    max_cpu_usage = max(avg_cpu_usage)\n",
        "    avg_cpu_overall = sum(avg_cpu_usage) / len(avg_cpu_usage)\n",
        "\n",
        "    print(f\"- Peak CPU usage: {max_cpu_usage:.1f}%\")\n",
        "    print(f\"- Average CPU usage: {avg_cpu_overall:.1f}%\")\n",
        "    print(f\"- CPU samples collected: {len(cpu_usage_data)}\")\n",
        "\n",
        "    # Show per-core usage at peak\n",
        "    peak_sample = max(cpu_usage_data, key=lambda x: x['avg_cpu'])\n",
        "    print(f\"\\nPer-core usage at peak:\")\n",
        "    for i, usage in enumerate(peak_sample['cpu_usage']):\n",
        "        print(f\"  Core {i}: {usage:.1f}%\")\n",
        "\n",
        "    # Visualize CPU usage\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Overall CPU usage over time\n",
        "    plt.subplot(2, 2, 1)\n",
        "    timestamps = [data['timestamp'] - cpu_usage_data[0]['timestamp'] for data in cpu_usage_data]\n",
        "    avg_usage = [data['avg_cpu'] for data in cpu_usage_data]\n",
        "    plt.plot(timestamps, avg_usage, 'b-', linewidth=2)\n",
        "    plt.title('Average CPU Usage Over Time')\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('CPU Usage (%)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Per-core usage heatmap\n",
        "    plt.subplot(2, 2, 2)\n",
        "    cpu_matrix = np.array([data['cpu_usage'] for data in cpu_usage_data]).T\n",
        "    plt.imshow(cpu_matrix, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
        "    plt.title('Per-Core CPU Usage Heatmap')\n",
        "    plt.xlabel('Time Samples')\n",
        "    plt.ylabel('CPU Core')\n",
        "    plt.colorbar(label='CPU Usage (%)')\n",
        "\n",
        "    # CPU usage distribution\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.hist(avg_usage, bins=20, alpha=0.7, edgecolor='black')\n",
        "    plt.title('CPU Usage Distribution')\n",
        "    plt.xlabel('CPU Usage (%)')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Parallel efficiency metrics\n",
        "    plt.subplot(2, 2, 4)\n",
        "    cores_active = [sum(1 for usage in data['cpu_usage'] if usage > 10) for data in cpu_usage_data]\n",
        "    plt.plot(timestamps, cores_active, 'g-', linewidth=2)\n",
        "    plt.title('Active CPU Cores (>10% usage)')\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('Number of Active Cores')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Parallel efficiency analysis\n",
        "    max_active_cores = max(cores_active)\n",
        "    avg_active_cores = sum(cores_active) / len(cores_active)\n",
        "    total_cores = psutil.cpu_count(logical=True)\n",
        "\n",
        "    print(f\"\\nParallel Processing Efficiency:\")\n",
        "    print(f\"- Maximum cores utilized: {max_active_cores}/{total_cores} ({max_active_cores/total_cores*100:.1f}%)\")\n",
        "    print(f\"- Average cores utilized: {avg_active_cores:.1f}/{total_cores} ({avg_active_cores/total_cores*100:.1f}%)\")\n",
        "\n",
        "    if max_active_cores > 1:\n",
        "        print(f\"\\n✅ PARALLEL PROCESSING CONFIRMED!\")\n",
        "        print(f\"   Multiple CPU cores were actively utilized during processing.\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️  Limited parallel utilization detected.\")\n",
        "        print(f\"   Consider increasing workload size or adjusting Spark configuration.\")\n",
        "\n",
        "# Compare with sequential processing\n",
        "print(f\"\\n\" + \"=\" * 50)\n",
        "print(\"SEQUENTIAL vs PARALLEL COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Sequential processing simulation\n",
        "sequential_start = time.time()\n",
        "sample_data = demo_data.sample(fraction=0.1, seed=42).toPandas()  # Small sample for sequential\n",
        "\n",
        "sequential_results = []\n",
        "for store_item, group in sample_data.groupby('store_item'):\n",
        "    result = {\n",
        "        'store_item': store_item,\n",
        "        'total_sales': group['sales'].sum(),\n",
        "        'avg_sales': group['sales'].mean(),\n",
        "        'record_count': len(group)\n",
        "    }\n",
        "    sequential_results.append(result)\n",
        "\n",
        "sequential_end = time.time()\n",
        "\n",
        "sequential_time = sequential_end - sequential_start\n",
        "parallel_time = parallel_end - parallel_start\n",
        "\n",
        "print(f\"Sequential processing time: {sequential_time:.3f} seconds\")\n",
        "print(f\"Parallel processing time: {parallel_time:.3f} seconds\")\n",
        "\n",
        "if sequential_time > parallel_time:\n",
        "    speedup = sequential_time / parallel_time\n",
        "    print(f\"Speedup achieved: {speedup:.2f}x faster with parallel processing\")\n",
        "else:\n",
        "    print(f\"Note: For small datasets, parallel overhead may exceed benefits\")\n",
        "\n",
        "print(\"\\n📸 Screenshot this cell output for Question 3 submission!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzhx_bg0b49T"
      },
      "source": [
        "### Additional Parallel Processing Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGWUFUE-b49T"
      },
      "outputs": [],
      "source": [
        "# Additional verification of parallel processing\n",
        "print(\"=\" * 60)\n",
        "print(\"SPARK EXECUTION PLAN ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Show Spark execution plan\n",
        "print(\"\\nSpark Physical Plan for Parallel Processing:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create a simple query to show execution plan\n",
        "demo_data.createOrReplaceTempView(\"demo_sales\")\n",
        "execution_plan = spark.sql(\"\"\"\n",
        "    SELECT store_item,\n",
        "           COUNT(*) as record_count,\n",
        "           SUM(sales) as total_sales,\n",
        "           AVG(sales) as avg_sales\n",
        "    FROM demo_sales\n",
        "    GROUP BY store_item\n",
        "\"\"\").explain()\n",
        "\n",
        "# Show task distribution\n",
        "print(f\"\\nSpark Task Distribution:\")\n",
        "print(f\"- Application ID: {spark.sparkContext.applicationId}\")\n",
        "print(f\"- Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
        "print(f\"- Executor Memory: {spark.conf.get('spark.driver.memory')}\")\n",
        "print(f\"- Serializer: {spark.conf.get('spark.serializer')}\")\n",
        "\n",
        "# Demonstrate partition-level processing\n",
        "print(f\"\\nPartition-Level Processing Verification:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "def partition_info(index, partition_data):\n",
        "    \"\"\"Return information about partition processing\"\"\"\n",
        "    import os\n",
        "    import threading\n",
        "\n",
        "    partition_list = list(partition_data)\n",
        "    return [{\n",
        "        'partition_id': index,\n",
        "        'process_id': os.getpid(),\n",
        "        'thread_id': threading.get_ident(),\n",
        "        'record_count': len(partition_list),\n",
        "        'timestamp': time.time()\n",
        "    }]\n",
        "\n",
        "# Get partition processing information\n",
        "partition_info_results = demo_data.rdd.mapPartitionsWithIndex(partition_info).collect()\n",
        "\n",
        "print(f\"Partition Processing Details:\")\n",
        "for info in partition_info_results:\n",
        "    print(f\"  Partition {info['partition_id']}: PID={info['process_id']}, \"\n",
        "          f\"Thread={info['thread_id']}, Records={info['record_count']}\")\n",
        "\n",
        "# Check if multiple processes/threads were used\n",
        "unique_pids = set(info['process_id'] for info in partition_info_results)\n",
        "unique_threads = set(info['thread_id'] for info in partition_info_results)\n",
        "\n",
        "print(f\"\\nParallel Execution Evidence:\")\n",
        "print(f\"- Unique Process IDs: {len(unique_pids)} ({unique_pids})\")\n",
        "print(f\"- Unique Thread IDs: {len(unique_threads)}\")\n",
        "print(f\"- Total Partitions Processed: {len(partition_info_results)}\")\n",
        "\n",
        "if len(unique_threads) > 1 or len(unique_pids) > 1:\n",
        "    print(f\"\\n✅ CONFIRMED: Multiple threads/processes utilized for parallel execution!\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  Single thread execution detected (may be due to small dataset size)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jucw0j0ab49U"
      },
      "source": [
        "### Cleanup and Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdAUiwmdb49U"
      },
      "outputs": [],
      "source": [
        "# Final summary and cleanup\n",
        "print(\"=\" * 80)\n",
        "print(\"NOTEBOOK EXECUTION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n📊 Data Processing Summary:\")\n",
        "print(f\"   • Total records processed: {store_item_history.count():,}\")\n",
        "print(f\"   • Store-item combinations: {store_item_combinations}\")\n",
        "print(f\"   • Successful forecasts: {successful_forecasts}\")\n",
        "print(f\"   • Processing time: {processing_time:.2f} seconds\")\n",
        "\n",
        "print(f\"\\n🔧 Technical Configuration:\")\n",
        "print(f\"   • Spark version: {spark.version}\")\n",
        "print(f\"   • Partitions used: {num_partitions}\")\n",
        "print(f\"   • CPU cores available: {psutil.cpu_count(logical=True)}\")\n",
        "print(f\"   • Default parallelism: {spark.sparkContext.defaultParallelism}\")\n",
        "\n",
        "print(f\"\\n📁 Output Files Created:\")\n",
        "print(f\"   • {output_dir}/forecast_results.json\")\n",
        "print(f\"   • {output_dir}/evaluation_results.json\")\n",
        "print(f\"   • {output_dir}/evaluation_summary.csv\")\n",
        "\n",
        "print(f\"\\n📋 Questions Addressed:\")\n",
        "print(f\"   ✅ Question 1: Parallel demand forecasting implementation completed\")\n",
        "print(f\"   ✅ Question 2: Partition count displayed ({num_partitions} partitions)\")\n",
        "print(f\"   ✅ Question 3: Parallel processing utilization demonstrated\")\n",
        "\n",
        "print(f\"\\n📸 Remember to take screenshots of:\")\n",
        "print(f\"   • Question 2 cell output (partition information)\")\n",
        "print(f\"   • Question 3 cell output (parallel processing demonstration)\")\n",
        "print(f\"   • This summary cell\")\n",
        "\n",
        "print(f\"\\n🎯 Key Achievements:\")\n",
        "print(f\"   • Successfully implemented parallel Prophet forecasting\")\n",
        "print(f\"   • Demonstrated PySpark distributed processing capabilities\")\n",
        "print(f\"   • Created comprehensive evaluation metrics\")\n",
        "print(f\"   • Persisted results for further analysis\")\n",
        "print(f\"   • Verified parallel execution with CPU monitoring\")\n",
        "\n",
        "# Clean up Spark session\n",
        "print(f\"\\n🧹 Cleaning up Spark session...\")\n",
        "spark.stop()\n",
        "print(f\"   Spark session stopped successfully.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY! 🎉\")\n",
        "print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}