{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 4.3: Koalas Random Forest Implementation\n",
        "\n",
        "This notebook revisits the PySpark Random Forest implementation from Question 4.1 and replaces as much PySpark code as possible with the Koalas API, which provides a pandas-like interface for Spark DataFrames.\n",
        "\n",
        "## Overview\n",
        "- Use Koalas for data loading and manipulation (pandas-like syntax)\n",
        "- Leverage Koalas for feature engineering and preprocessing\n",
        "- Convert to PySpark DataFrame only when necessary for MLlib\n",
        "- Train Random Forest using PySpark MLlib (as Koalas doesn't have ML algorithms)\n",
        "- Compare the code simplicity and readability with pure PySpark approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.pandas as ps  # Koalas is now integrated as pyspark.pandas\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Imputer, StandardScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Configure Koalas to work with larger datasets\n",
        "ps.set_option('compute.default_index_type', 'distributed')\n",
        "ps.set_option('compute.ops_on_diff_frames', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RandomForestKoalas\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Koalas version: {ps.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading with Koalas\n",
        "Use Koalas to load data with pandas-like syntax, making the code more familiar and readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data files (same as original implementation)\n",
        "import subprocess\n",
        "\n",
        "# Download external sources\n",
        "subprocess.run([\n",
        "    \"wget\", \n",
        "    \"https://storage.googleapis.com/bdt-spark-store/external_sources.csv\", \n",
        "    \"-O\", \"gcs_external_sources.csv\"\n",
        "], check=True)\n",
        "\n",
        "# Download internal data\n",
        "subprocess.run([\n",
        "    \"wget\", \n",
        "    \"https://storage.googleapis.com/bdt-spark-store/internal_data.csv\", \n",
        "    \"-O\", \"gcs_internal_data.csv\"\n",
        "], check=True)\n",
        "\n",
        "print(\"Data files downloaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data using Koalas (pandas-like syntax)\n",
        "df_data = ps.read_csv(\"gcs_internal_data.csv\")\n",
        "df_ext = ps.read_csv(\"gcs_external_sources.csv\")\n",
        "\n",
        "print(f\"Internal data shape: {df_data.shape}\")\n",
        "print(f\"External data shape: {df_ext.shape}\")\n",
        "\n",
        "# Show basic info (pandas-like)\n",
        "print(\"\\nInternal data info:\")\n",
        "print(df_data.dtypes.head(10))\n",
        "print(\"\\nExternal data info:\")\n",
        "print(df_ext.dtypes.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Joining with Koalas\n",
        "Use pandas-like merge syntax instead of PySpark join operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join datasets using pandas-like merge syntax\n",
        "df_full = df_data.merge(df_ext, on=\"SK_ID_CURR\", how=\"inner\")\n",
        "\n",
        "print(f\"Joined data shape: {df_full.shape}\")\n",
        "\n",
        "# Show first few rows (pandas-like)\n",
        "print(\"\\nFirst 3 rows:\")\n",
        "print(df_full.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Selection with Koalas\n",
        "Use pandas-like column selection syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the same columns as in the original implementation (pandas-like syntax)\n",
        "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
        "                  'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
        "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "                  'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
        "                  'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
        "\n",
        "df = df_full[columns_extract]  # pandas-like column selection\n",
        "\n",
        "print(f\"Selected features shape: {df.shape}\")\n",
        "print(\"\\nFirst 3 rows of selected features:\")\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration with Koalas\n",
        "Use pandas-like methods for data exploration and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore data using pandas-like methods\n",
        "print(\"Data types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nTarget distribution:\")\n",
        "print(df['TARGET'].value_counts())\n",
        "\n",
        "print(\"\\nBasic statistics for numerical columns:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train-Test Split with Koalas\n",
        "Use pandas-like sampling for train-test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train-test split using pandas-like sampling\n",
        "# Note: Koalas doesn't have train_test_split, so we use sampling\n",
        "train_df_koalas = df.sample(frac=0.8, random_state=101)\n",
        "test_df_koalas = df.drop(train_df_koalas.index)\n",
        "\n",
        "print(f\"Training set shape: {train_df_koalas.shape}\")\n",
        "print(f\"Test set shape: {test_df_koalas.shape}\")\n",
        "\n",
        "# Check target distribution using pandas-like syntax\n",
        "print(\"\\nTraining set target distribution:\")\n",
        "print(train_df_koalas['TARGET'].value_counts())\n",
        "\n",
        "print(\"\\nTest set target distribution:\")\n",
        "print(test_df_koalas['TARGET'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering with Koalas\n",
        "Use pandas-like operations for feature engineering including dummy encoding and data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['NAME_EDUCATION_TYPE', 'CODE_GENDER', 'ORGANIZATION_TYPE', 'NAME_INCOME_TYPE']\n",
        "numerical_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
        "                 'DAYS_ID_PUBLISH', 'AMT_ANNUITY', 'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', \n",
        "                 'AMT_CREDIT', 'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE']\n",
        "\n",
        "print(f\"Categorical columns: {categorical_cols}\")\n",
        "print(f\"Numerical columns: {numerical_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encoding using pandas-like get_dummies (much simpler than PySpark!)\n",
        "train_encoded = ps.get_dummies(train_df_koalas, columns=categorical_cols, prefix=categorical_cols)\n",
        "test_encoded = ps.get_dummies(test_df_koalas, columns=categorical_cols, prefix=categorical_cols)\n",
        "\n",
        "print(f\"Training set shape after encoding: {train_encoded.shape}\")\n",
        "print(f\"Test set shape after encoding: {test_encoded.shape}\")\n",
        "\n",
        "# Align columns between train and test (pandas-like operation)\n",
        "# Get common columns\n",
        "train_cols = set(train_encoded.columns)\n",
        "test_cols = set(test_encoded.columns)\n",
        "common_cols = list(train_cols.intersection(test_cols))\n",
        "\n",
        "# Ensure TARGET is included\n",
        "if 'TARGET' not in common_cols:\n",
        "    common_cols.append('TARGET')\n",
        "\n",
        "train_aligned = train_encoded[common_cols]\n",
        "test_aligned = test_encoded[common_cols]\n",
        "\n",
        "print(f\"Aligned training set shape: {train_aligned.shape}\")\n",
        "print(f\"Aligned test set shape: {test_aligned.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values using pandas-like fillna\n",
        "# Calculate median for numerical columns in training set\n",
        "numerical_medians = {}\n",
        "for col in numerical_cols:\n",
        "    if col in train_aligned.columns:\n",
        "        numerical_medians[col] = train_aligned[col].median()\n",
        "\n",
        "print(\"Numerical medians for imputation:\")\n",
        "for col, median_val in numerical_medians.items():\n",
        "    print(f\"{col}: {median_val}\")\n",
        "\n",
        "# Fill missing values with median (pandas-like)\n",
        "train_imputed = train_aligned.fillna(numerical_medians)\n",
        "test_imputed = test_aligned.fillna(numerical_medians)\n",
        "\n",
        "print(f\"\\nTraining set shape after imputation: {train_imputed.shape}\")\n",
        "print(f\"Test set shape after imputation: {test_imputed.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert to PySpark for MLlib\n",
        "Convert Koalas DataFrames to PySpark DataFrames for machine learning with MLlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert Koalas DataFrames to PySpark DataFrames for MLlib\n",
        "train_spark = train_imputed.to_spark()\n",
        "test_spark = test_imputed.to_spark()\n",
        "\n",
        "print(f\"Converted to PySpark - Training: {train_spark.count()} rows, {len(train_spark.columns)} columns\")\n",
        "print(f\"Converted to PySpark - Test: {test_spark.count()} rows, {len(test_spark.columns)} columns\")\n",
        "\n",
        "# Prepare feature columns (exclude TARGET)\n",
        "feature_columns = [col for col in train_spark.columns if col != 'TARGET']\n",
        "print(f\"\\nNumber of feature columns: {len(feature_columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training with PySpark MLlib\n",
        "Use PySpark MLlib for the actual machine learning (as Koalas doesn't provide ML algorithms)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create feature vector and scaling pipeline\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_unscaled\")\n",
        "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", \n",
        "                       withStd=True, withMean=True)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TARGET\",\n",
        "    numTrees=100,  # equivalent to n_estimators=100\n",
        "    seed=50,       # equivalent to random_state=50\n",
        "    maxDepth=10,\n",
        "    minInstancesPerNode=1\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
        "\n",
        "print(\"Training the model...\")\n",
        "model = pipeline.fit(train_spark)\n",
        "print(\"Model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "Evaluate the model performance using the same metrics as in the PySpark implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "predictions = model.transform(test_spark)\n",
        "\n",
        "# Show predictions\n",
        "predictions.select(\"TARGET\", \"prediction\", \"probability\").show(10)\n",
        "\n",
        "# Calculate accuracy\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"TARGET\", \n",
        "    predictionCol=\"prediction\", \n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"\\nAccuracy: {accuracy:.10f}\")\n",
        "\n",
        "# Additional metrics\n",
        "f1_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"TARGET\", \n",
        "    predictionCol=\"prediction\", \n",
        "    metricName=\"f1\"\n",
        ")\n",
        "f1_score = f1_evaluator.evaluate(predictions)\n",
        "print(f\"F1 Score: {f1_score:.10f}\")\n",
        "\n",
        "# AUC for binary classification\n",
        "auc_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"TARGET\", \n",
        "    rawPredictionCol=\"rawPrediction\", \n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "auc = auc_evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc:.10f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance Analysis\n",
        "Extract and analyze feature importance from the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Random Forest model from the pipeline\n",
        "rf_model = model.stages[-1]\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_model.featureImportances.toArray()\n",
        "\n",
        "# Create feature importance analysis using Koalas (pandas-like)\n",
        "importance_data = {\n",
        "    'feature_name': feature_columns,\n",
        "    'importance': feature_importances\n",
        "}\n",
        "\n",
        "# Convert to Koalas DataFrame for easier manipulation\n",
        "importance_df = ps.DataFrame(importance_data)\n",
        "\n",
        "# Sort by importance (pandas-like syntax)\n",
        "importance_sorted = importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 20 most important features:\")\n",
        "print(importance_sorted.head(20))\n",
        "\n",
        "print(f\"\\nTotal number of features: {len(feature_importances)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Persistence\n",
        "Save the trained model to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the complete pipeline model\n",
        "model_path = \"./koalas_random_forest_model\"\n",
        "model.write().overwrite().save(model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "# Also save just the Random Forest model\n",
        "rf_model_path = \"./koalas_rf_only_model\"\n",
        "rf_model.write().overwrite().save(rf_model_path)\n",
        "print(f\"Random Forest model saved to: {rf_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison: Koalas vs Pure PySpark\n",
        "\n",
        "### Advantages of Using Koalas:\n",
        "\n",
        "1. **Familiar Syntax**: Koalas provides pandas-like syntax, making it easier for data scientists familiar with pandas to work with Spark DataFrames.\n",
        "\n",
        "2. **Simplified Data Manipulation**:\n",
        "   - `df.merge()` instead of `df.join()`\n",
        "   - `ps.get_dummies()` instead of complex StringIndexer + OneHotEncoder pipeline\n",
        "   - `df.fillna()` instead of Imputer transformations\n",
        "   - `df.sample()` for train-test split instead of `randomSplit()`\n",
        "\n",
        "3. **Easier Data Exploration**:\n",
        "   - `df.describe()`, `df.value_counts()`, `df.head()` work as expected\n",
        "   - More intuitive data inspection and analysis\n",
        "\n",
        "4. **Reduced Code Complexity**: The Koalas implementation requires significantly fewer lines of code for data preprocessing.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "1. **No ML Algorithms**: Koalas doesn't provide machine learning algorithms, so we still need to convert to PySpark DataFrames for MLlib.\n",
        "\n",
        "2. **Performance Considerations**: Some Koalas operations might be less optimized than native PySpark operations.\n",
        "\n",
        "3. **Feature Completeness**: Not all pandas features are available in Koalas.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Koalas significantly simplifies data preprocessing and exploration tasks, making the code more readable and maintainable. However, for machine learning tasks, we still need to rely on PySpark MLlib. The hybrid approach (Koalas for data processing + PySpark MLlib for ML) provides the best of both worlds: familiar syntax for data manipulation and powerful distributed ML capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the class imbalance issue using Koalas\n",
        "print(\"Class distribution analysis using Koalas:\")\n",
        "\n",
        "# Convert predictions back to Koalas for easier analysis\n",
        "predictions_koalas = predictions.select(\"TARGET\", \"prediction\").to_koalas()\n",
        "\n",
        "print(\"\\nActual vs Predicted distribution:\")\n",
        "print(\"Actual TARGET distribution:\")\n",
        "print(predictions_koalas['TARGET'].value_counts())\n",
        "\n",
        "print(\"\\nPredicted distribution:\")\n",
        "print(predictions_koalas['prediction'].value_counts())\n",
        "\n",
        "# Calculate baseline accuracy\n",
        "total_samples = len(predictions_koalas)\n",
        "majority_class_count = predictions_koalas['TARGET'].value_counts().max()\n",
        "baseline_accuracy = majority_class_count / total_samples\n",
        "\n",
        "print(f\"\\nBaseline accuracy (always predict majority class): {baseline_accuracy:.4f}\")\n",
        "print(f\"Our model accuracy: {accuracy:.4f}\")\n",
        "print(f\"Improvement over baseline: {accuracy - baseline_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}