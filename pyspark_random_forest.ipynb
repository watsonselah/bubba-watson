{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 4.1: PySpark Random Forest Implementation\n",
        "\n",
        "This notebook ports the pandas Random Forest implementation to PySpark, using only PySpark APIs for data processing and MLlib for machine learning.\n",
        "\n",
        "## Overview\n",
        "- Load data from Google Cloud Storage\n",
        "- Join datasets using PySpark DataFrame operations\n",
        "- Perform feature engineering with PySpark\n",
        "- Train Random Forest using PySpark MLlib\n",
        "- Evaluate model and save to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Imputer, StandardScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RandomForestPySpark\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark context: {spark.sparkContext}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n",
        "Load the same datasets from Google Cloud Storage as in the original implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data files (same as original implementation)\n",
        "import subprocess\n",
        "\n",
        "# Download external sources\n",
        "subprocess.run([\n",
        "    \"wget\", \n",
        "    \"https://storage.googleapis.com/bdt-spark-store/external_sources.csv\", \n",
        "    \"-O\", \"gcs_external_sources.csv\"\n",
        "], check=True)\n",
        "\n",
        "# Download internal data\n",
        "subprocess.run([\n",
        "    \"wget\", \n",
        "    \"https://storage.googleapis.com/bdt-spark-store/internal_data.csv\", \n",
        "    \"-O\", \"gcs_internal_data.csv\"\n",
        "], check=True)\n",
        "\n",
        "print(\"Data files downloaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data using PySpark\n",
        "df_data = spark.read.csv(\"gcs_internal_data.csv\", header=True, inferSchema=True)\n",
        "df_ext = spark.read.csv(\"gcs_external_sources.csv\", header=True, inferSchema=True)\n",
        "\n",
        "print(f\"Internal data shape: {df_data.count()} rows, {len(df_data.columns)} columns\")\n",
        "print(f\"External data shape: {df_ext.count()} rows, {len(df_ext.columns)} columns\")\n",
        "\n",
        "# Show schema\n",
        "print(\"\\nInternal data schema:\")\n",
        "df_data.printSchema()\n",
        "print(\"\\nExternal data schema:\")\n",
        "df_ext.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Joining\n",
        "Join the datasets on their common identifier key using PySpark DataFrame operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join datasets on SK_ID_CURR (equivalent to pandas merge)\n",
        "df_full = df_data.join(df_ext, on=\"SK_ID_CURR\", how=\"inner\")\n",
        "\n",
        "print(f\"Joined data shape: {df_full.count()} rows, {len(df_full.columns)} columns\")\n",
        "\n",
        "# Show first few rows\n",
        "df_full.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Selection\n",
        "Select the same features as in the original implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the same columns as in the original implementation\n",
        "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
        "                  'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
        "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "                  'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
        "                  'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
        "\n",
        "df = df_full.select(*columns_extract)\n",
        "\n",
        "print(f\"Selected features shape: {df.count()} rows, {len(df.columns)} columns\")\n",
        "df.show(3, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train-Test Split\n",
        "Split the data into training and testing sets using PySpark's randomSplit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seed for reproducibility (equivalent to np.random.RandomState(101))\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoint\")\n",
        "\n",
        "# Split data 80/20 for train/test (equivalent to the original 0.8 split)\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=101)\n",
        "\n",
        "print(f\"Training set: {train_df.count()} rows\")\n",
        "print(f\"Test set: {test_df.count()} rows\")\n",
        "\n",
        "# Check target distribution\n",
        "print(\"\\nTraining set target distribution:\")\n",
        "train_df.groupBy(\"TARGET\").count().show()\n",
        "\n",
        "print(\"Test set target distribution:\")\n",
        "test_df.groupBy(\"TARGET\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering Pipeline\n",
        "Create a PySpark ML Pipeline for preprocessing steps including categorical encoding, imputation, and scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['NAME_EDUCATION_TYPE', 'CODE_GENDER', 'ORGANIZATION_TYPE', 'NAME_INCOME_TYPE']\n",
        "numerical_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
        "                 'DAYS_ID_PUBLISH', 'AMT_ANNUITY', 'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', \n",
        "                 'AMT_CREDIT', 'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE']\n",
        "\n",
        "print(f\"Categorical columns: {categorical_cols}\")\n",
        "print(f\"Numerical columns: {numerical_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create preprocessing pipeline stages\n",
        "stages = []\n",
        "\n",
        "# String indexing for categorical variables\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"keep\") \n",
        "           for col in categorical_cols]\n",
        "stages.extend(indexers)\n",
        "\n",
        "# One-hot encoding for categorical variables (equivalent to pd.get_dummies)\n",
        "encoders = [OneHotEncoder(inputCol=col + \"_indexed\", outputCol=col + \"_encoded\") \n",
        "           for col in categorical_cols]\n",
        "stages.extend(encoders)\n",
        "\n",
        "# Imputation for numerical columns (median strategy)\n",
        "imputer = Imputer(inputCols=numerical_cols, \n",
        "                 outputCols=[col + \"_imputed\" for col in numerical_cols],\n",
        "                 strategy=\"median\")\n",
        "stages.append(imputer)\n",
        "\n",
        "# Prepare feature columns for vector assembler\n",
        "encoded_categorical_cols = [col + \"_encoded\" for col in categorical_cols]\n",
        "imputed_numerical_cols = [col + \"_imputed\" for col in numerical_cols]\n",
        "feature_cols = encoded_categorical_cols + imputed_numerical_cols\n",
        "\n",
        "# Vector assembler to combine all features\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_unscaled\")\n",
        "stages.append(assembler)\n",
        "\n",
        "# Standard scaling (equivalent to StandardScaler in sklearn)\n",
        "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", \n",
        "                       withStd=True, withMean=True)\n",
        "stages.append(scaler)\n",
        "\n",
        "print(f\"Created preprocessing pipeline with {len(stages)} stages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "Train the Random Forest model using PySpark MLlib with the same parameters as the original implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest Classifier (equivalent to sklearn RandomForestClassifier)\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TARGET\",\n",
        "    numTrees=100,  # equivalent to n_estimators=100\n",
        "    seed=50,       # equivalent to random_state=50\n",
        "    maxDepth=10,   # reasonable default for large datasets\n",
        "    minInstancesPerNode=1\n",
        ")\n",
        "\n",
        "# Add Random Forest to pipeline\n",
        "stages.append(rf)\n",
        "\n",
        "# Create and fit the complete pipeline\n",
        "pipeline = Pipeline(stages=stages)\n",
        "\n",
        "print(\"Training the model...\")\n",
        "model = pipeline.fit(train_df)\n",
        "print(\"Model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "Make predictions and calculate accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "# Show predictions\n",
        "predictions.select(\"TARGET\", \"prediction\", \"probability\").show(10)\n",
        "\n",
        "# Calculate accuracy (equivalent to sklearn accuracy_score)\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"TARGET\", \n",
        "    predictionCol=\"prediction\", \n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"\\nAccuracy: {accuracy:.10f}\")\n",
        "\n",
        "# Additional metrics\n",
        "f1_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"TARGET\", \n",
        "    predictionCol=\"prediction\", \n",
        "    metricName=\"f1\"\n",
        ")\n",
        "f1_score = f1_evaluator.evaluate(predictions)\n",
        "print(f\"F1 Score: {f1_score:.10f}\")\n",
        "\n",
        "# AUC for binary classification\n",
        "auc_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"TARGET\", \n",
        "    rawPredictionCol=\"rawPrediction\", \n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "auc = auc_evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc:.10f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance\n",
        "Extract and display feature importance from the trained Random Forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the Random Forest model from the pipeline\n",
        "rf_model = model.stages[-1]\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_model.featureImportances.toArray()\n",
        "\n",
        "# Create feature importance DataFrame\n",
        "# Note: feature names correspond to the assembled features\n",
        "importance_data = list(zip(range(len(feature_importances)), feature_importances))\n",
        "importance_df = spark.createDataFrame(importance_data, [\"feature_index\", \"importance\"])\n",
        "\n",
        "# Sort by importance and show top features\n",
        "importance_df.orderBy(col(\"importance\").desc()).show(20)\n",
        "\n",
        "print(f\"\\nTotal number of features after preprocessing: {len(feature_importances)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Persistence\n",
        "Save the trained model to disk for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the complete pipeline model\n",
        "model_path = \"./pyspark_random_forest_model\"\n",
        "model.write().overwrite().save(model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "# Also save just the Random Forest model\n",
        "rf_model_path = \"./pyspark_rf_only_model\"\n",
        "rf_model.write().overwrite().save(rf_model_path)\n",
        "print(f\"Random Forest model saved to: {rf_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4.2: Accuracy Metric Analysis\n",
        "\n",
        "### Is accuracy a good choice of metric for this problem?\n",
        "\n",
        "**Answer: No, accuracy is not the best metric for this problem.**\n",
        "\n",
        "**Reasoning:**\n",
        "\n",
        "1. **Class Imbalance**: The dataset shows a severe class imbalance with approximately:\n",
        "   - Class 0 (no default): ~91.9%\n",
        "   - Class 1 (default): ~8.1%\n",
        "\n",
        "2. **Accuracy Paradox**: With such imbalance, a naive classifier that always predicts class 0 would achieve ~91.9% accuracy without learning anything meaningful about the data.\n",
        "\n",
        "3. **Business Context**: In credit risk assessment, the cost of missing a default (false negative) is typically much higher than incorrectly flagging a good customer (false positive). Accuracy treats both errors equally.\n",
        "\n",
        "**Better Metrics for This Problem:**\n",
        "\n",
        "1. **Precision and Recall for Class 1**: Focus on how well we identify actual defaults\n",
        "2. **F1-Score**: Harmonic mean of precision and recall, better for imbalanced datasets\n",
        "3. **AUC-ROC**: Measures the model's ability to distinguish between classes across all thresholds\n",
        "4. **Precision-Recall AUC**: Particularly useful for imbalanced datasets\n",
        "5. **Cost-sensitive metrics**: Incorporate business costs of different types of errors\n",
        "\n",
        "**Conclusion**: While accuracy provides a quick overview, it can be misleading for imbalanced datasets like this credit risk problem. The F1-score and AUC metrics calculated above provide more meaningful insights into model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the class imbalance issue\n",
        "print(\"Class distribution in test set:\")\n",
        "class_counts = test_df.groupBy(\"TARGET\").count().collect()\n",
        "total_count = test_df.count()\n",
        "\n",
        "for row in class_counts:\n",
        "    class_label = row['TARGET']\n",
        "    count = row['count']\n",
        "    percentage = (count / total_count) * 100\n",
        "    print(f\"Class {class_label}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "# Calculate baseline accuracy (always predict majority class)\n",
        "majority_class_count = max([row['count'] for row in class_counts])\n",
        "baseline_accuracy = majority_class_count / total_count\n",
        "print(f\"\\nBaseline accuracy (always predict majority class): {baseline_accuracy:.4f}\")\n",
        "print(f\"Our model accuracy: {accuracy:.4f}\")\n",
        "print(f\"Improvement over baseline: {accuracy - baseline_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}