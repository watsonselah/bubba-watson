{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/watsonselah/bubba-watson/blob/master/koalas_random_forest_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import-libraries"
      },
      "source": [
        "## 1. Import Libraries\n",
        "Import required libraries including pyspark.pandas (Koalas), PySpark MLlib, and other dependencies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "qVIk2oyQ7DRh",
        "outputId": "b332daee-8823-4d1a-c596-25dd7218e10e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy 2.0 & PyArrow Compatibility Setup\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "# Set PyArrow environment variable to suppress timezone warnings\n",
        "os.environ['PYARROW_IGNORE_TIMEZONE'] = '1'\n",
        "print(\"✓ PyArrow timezone environment variable set\")\n",
        "\n",
        "# NumPy 2.0 compatibility fix\n",
        "try:\n",
        "    import numpy as np\n",
        "\n",
        "    # Check if np.NaN exists (it was removed in NumPy 2.0)\n",
        "    if not hasattr(np, 'NaN'):\n",
        "        # Add np.NaN as an alias to np.nan for backward compatibility\n",
        "        np.NaN = np.nan\n",
        "\n",
        "    print(f\"NumPy version: {np.__version__}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ NumPy compatibility setup failed: {e}\")\n",
        "    print(\"Continuing with standard NumPy import...\")"
      ],
      "metadata": {
        "id": "PBbWt2OXzkYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DLPiTsdVXM9x",
        "outputId": "b5671e26-4267-487f-a9ac-13652a1a9c32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.pandas as ps  # Koalas is now integrated as pyspark.pandas\n",
        "from pyspark.pandas import config\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Imputer, StandardScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import subprocess\n",
        "\n",
        "# Configure Koalas to work with larger datasets\n",
        "ps.set_option('compute.default_index_type', 'distributed')\n",
        "ps.set_option('compute.ops_on_diff_frames', True)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spark-setup"
      },
      "source": [
        "## 2. Spark Session Setup\n",
        "Initialize Spark session with optimized configuration for Koalas operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PLkdLTBRXM9y",
        "outputId": "32931830-5464-4d8f-ccc7-0744d1837150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 3.5.1\n",
            "Spark context: <SparkContext master=local[*] appName=pandas-on-Spark>\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark Session with optimized configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RandomForestKoalas\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark context: {spark.sparkContext}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-loading"
      },
      "source": [
        "## 3. Data Loading with Koalas\n",
        "Use Koalas to load data with pandas-like syntax, making the code more familiar and readable compared to PySpark's DataFrame API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GsR1IPhbXM9z",
        "outputId": "7cd31b29-8dba-4d9c-9eb5-69afe8226db2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data files...\n",
            "Data files downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Download data files (same as original implementation)\n",
        "print(\"Downloading data files...\")\n",
        "\n",
        "# Download external sources\n",
        "subprocess.run([\n",
        "    \"wget\",\n",
        "    \"https://storage.googleapis.com/bdt-spark-store/external_sources.csv\",\n",
        "    \"-O\", \"gcs_external_sources.csv\"\n",
        "], check=True)\n",
        "\n",
        "# Download internal data\n",
        "subprocess.run([\n",
        "    \"wget\",\n",
        "    \"https://storage.googleapis.com/bdt-spark-store/internal_data.csv\",\n",
        "    \"-O\", \"gcs_internal_data.csv\"\n",
        "], check=True)\n",
        "\n",
        "print(\"Data files downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VJX9f6gIXM9z",
        "outputId": "d42bf194-a3d7-413f-91f2-46a9e40e1e74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data with Koalas...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Internal data shape: (307511, 119)\n",
            "External data shape: (307511, 4)\n",
            "\n",
            "Internal data column types (first 10):\n",
            "SK_ID_CURR              int32\n",
            "TARGET                  int32\n",
            "NAME_CONTRACT_TYPE     object\n",
            "CODE_GENDER            object\n",
            "FLAG_OWN_CAR           object\n",
            "FLAG_OWN_REALTY        object\n",
            "CNT_CHILDREN            int32\n",
            "AMT_INCOME_TOTAL      float64\n",
            "AMT_CREDIT            float64\n",
            "AMT_ANNUITY           float64\n",
            "dtype: object\n",
            "\n",
            "External data column types:\n",
            "SK_ID_CURR        int32\n",
            "EXT_SOURCE_1    float64\n",
            "EXT_SOURCE_2    float64\n",
            "EXT_SOURCE_3    float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Load data using Koalas\n",
        "print(\"Loading data with Koalas...\")\n",
        "\n",
        "df_data = ps.read_csv(\"gcs_internal_data.csv\")\n",
        "df_ext = ps.read_csv(\"gcs_external_sources.csv\")\n",
        "\n",
        "print(f\"Internal data shape: {df_data.shape}\")\n",
        "print(f\"External data shape: {df_ext.shape}\")\n",
        "\n",
        "# Show basic info using pandas-like methods\n",
        "print(\"\\nInternal data column types (first 10):\")\n",
        "print(df_data.dtypes.head(10))\n",
        "\n",
        "print(\"\\nExternal data column types:\")\n",
        "print(df_ext.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-joining"
      },
      "source": [
        "## 4. Data Joining with Koalas\n",
        "Use pandas-like merge syntax instead of PySpark join operations. This is much more intuitive for data scientists familiar with pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "To29MDl8XM9z",
        "outputId": "12230cec-8488-4102-bebb-20a6763e87f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joining datasets using Koalas merge...\n",
            "Joined data shape: (307511, 122)\n",
            "\n",
            "First 3 rows of joined data:\n",
            "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  NAME_TYPE_SUITE NAME_INCOME_TYPE            NAME_EDUCATION_TYPE    NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  DAYS_ID_PUBLISH  OWN_CAR_AGE  FLAG_MOBIL  FLAG_EMP_PHONE  FLAG_WORK_PHONE  FLAG_CONT_MOBILE  FLAG_PHONE  FLAG_EMAIL OCCUPATION_TYPE  CNT_FAM_MEMBERS  REGION_RATING_CLIENT  REGION_RATING_CLIENT_W_CITY WEEKDAY_APPR_PROCESS_START  HOUR_APPR_PROCESS_START  REG_REGION_NOT_LIVE_REGION  REG_REGION_NOT_WORK_REGION  LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY ORGANIZATION_TYPE  APARTMENTS_AVG  BASEMENTAREA_AVG  YEARS_BEGINEXPLUATATION_AVG  YEARS_BUILD_AVG  COMMONAREA_AVG  ELEVATORS_AVG  ENTRANCES_AVG  FLOORSMAX_AVG  FLOORSMIN_AVG  LANDAREA_AVG  LIVINGAPARTMENTS_AVG  LIVINGAREA_AVG  NONLIVINGAPARTMENTS_AVG  NONLIVINGAREA_AVG  APARTMENTS_MODE  BASEMENTAREA_MODE  YEARS_BEGINEXPLUATATION_MODE  YEARS_BUILD_MODE  COMMONAREA_MODE  ELEVATORS_MODE  ENTRANCES_MODE  FLOORSMAX_MODE  FLOORSMIN_MODE  LANDAREA_MODE  LIVINGAPARTMENTS_MODE  LIVINGAREA_MODE  NONLIVINGAPARTMENTS_MODE  NONLIVINGAREA_MODE  APARTMENTS_MEDI  BASEMENTAREA_MEDI  YEARS_BEGINEXPLUATATION_MEDI  YEARS_BUILD_MEDI  COMMONAREA_MEDI  ELEVATORS_MEDI  ENTRANCES_MEDI  FLOORSMAX_MEDI  FLOORSMIN_MEDI  LANDAREA_MEDI  LIVINGAPARTMENTS_MEDI  LIVINGAREA_MEDI  NONLIVINGAPARTMENTS_MEDI  NONLIVINGAREA_MEDI FONDKAPREMONT_MODE  HOUSETYPE_MODE  TOTALAREA_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE  OBS_30_CNT_SOCIAL_CIRCLE  DEF_30_CNT_SOCIAL_CIRCLE  OBS_60_CNT_SOCIAL_CIRCLE  DEF_60_CNT_SOCIAL_CIRCLE  DAYS_LAST_PHONE_CHANGE  FLAG_DOCUMENT_2  FLAG_DOCUMENT_3  FLAG_DOCUMENT_4  FLAG_DOCUMENT_5  FLAG_DOCUMENT_6  FLAG_DOCUMENT_7  FLAG_DOCUMENT_8  FLAG_DOCUMENT_9  FLAG_DOCUMENT_10  FLAG_DOCUMENT_11  FLAG_DOCUMENT_12  FLAG_DOCUMENT_13  FLAG_DOCUMENT_14  FLAG_DOCUMENT_15  FLAG_DOCUMENT_16  FLAG_DOCUMENT_17  FLAG_DOCUMENT_18  FLAG_DOCUMENT_19  FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  AMT_REQ_CREDIT_BUREAU_HOUR  AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3\n",
            "0      100003       0         Cash loans           F            N               N             0          270000.0   1293502.5      35698.5        1129500.0           Family    State servant               Higher education               Married  House / apartment                    0.003541      -16765          -1188            -1186.0             -291          NaN           1               1                0                 1           1           0      Core staff              2.0                     1                            1                     MONDAY                       11                           0                           0                            0                       0                       0                        0            School          0.0959            0.0529                       0.9851            0.796          0.0605           0.08         0.0345         0.2917         0.3333         0.013                0.0773          0.0549                   0.0039             0.0098           0.0924             0.0538                        0.9851             0.804           0.0497          0.0806          0.0345          0.2917          0.3333         0.0128                  0.079           0.0554                       0.0                 0.0           0.0968             0.0529                        0.9851            0.7987           0.0608            0.08          0.0345          0.2917          0.3333         0.0132                 0.0787           0.0558                    0.0039                0.01   reg oper account  block of flats          0.0714              Block                  No                       1.0                       0.0                       1.0                       0.0                  -828.0                0                1                0                0                0                0                0                0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                         0.0                        0.0                         0.0                        0.0                        0.0                         0.0      0.311267      0.622246           NaN\n",
            "1      100007       0         Cash loans           M            N               Y             0          121500.0    513000.0      21865.5         513000.0    Unaccompanied          Working  Secondary / secondary special  Single / not married  House / apartment                    0.028663      -19932          -3038            -4311.0            -3458          NaN           1               1                0                 1           0           0      Core staff              1.0                     2                            2                   THURSDAY                       11                           0                           0                            0                       0                       1                        1          Religion             NaN               NaN                          NaN              NaN             NaN            NaN            NaN            NaN            NaN           NaN                   NaN             NaN                      NaN                NaN              NaN                NaN                           NaN               NaN              NaN             NaN             NaN             NaN             NaN            NaN                    NaN              NaN                       NaN                 NaN              NaN                NaN                           NaN               NaN              NaN             NaN             NaN             NaN             NaN            NaN                    NaN              NaN                       NaN                 NaN               None            None             NaN               None                None                       0.0                       0.0                       0.0                       0.0                 -1106.0                0                0                0                0                0                0                1                0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                         0.0                        0.0                         0.0                        0.0                        0.0                         0.0           NaN      0.322738           NaN\n",
            "2      100008       0         Cash loans           M            N               Y             0           99000.0    490495.5      27517.5         454500.0  Spouse, partner    State servant  Secondary / secondary special               Married  House / apartment                    0.035792      -16941          -1588            -4970.0             -477          NaN           1               1                1                 1           1           0        Laborers              2.0                     2                            2                  WEDNESDAY                       16                           0                           0                            0                       0                       0                        0             Other             NaN               NaN                          NaN              NaN             NaN            NaN            NaN            NaN            NaN           NaN                   NaN             NaN                      NaN                NaN              NaN                NaN                           NaN               NaN              NaN             NaN             NaN             NaN             NaN            NaN                    NaN              NaN                       NaN                 NaN              NaN                NaN                           NaN               NaN              NaN             NaN             NaN             NaN             NaN            NaN                    NaN              NaN                       NaN                 NaN               None            None             NaN               None                None                       0.0                       0.0                       0.0                       0.0                 -2536.0                0                1                0                0                0                0                0                0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                 0                         0.0                        0.0                         0.0                        0.0                        1.0                         1.0           NaN      0.354225      0.621226\n",
            "\n",
            "Column names (first 10):\n",
            "['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY']\n"
          ]
        }
      ],
      "source": [
        "# Join datasets using pandas-like merge syntax (much simpler than PySpark join!)\n",
        "print(\"Joining datasets using Koalas merge...\")\n",
        "\n",
        "df_full = df_data.merge(df_ext, on=\"SK_ID_CURR\", how=\"inner\")\n",
        "\n",
        "print(f\"Joined data shape: {df_full.shape}\")\n",
        "\n",
        "# Show first few rows using pandas-like head() method\n",
        "print(\"\\nFirst 3 rows of joined data:\")\n",
        "print(df_full.head(3))\n",
        "\n",
        "print(\"\\nColumn names (first 10):\")\n",
        "print(df_full.columns[:10].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-selection"
      },
      "source": [
        "## 5. Feature Selection with Koalas\n",
        "Use pandas-like column selection syntax. This is identical to pandas, making it very familiar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "feature-selection-code",
        "outputId": "5c4df1f4-fb78-437a-aa94-9d858144cca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features shape: (307511, 18)\n",
            "Selected columns: 18\n",
            "\n",
            "First 3 rows of selected features:\n",
            "   EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  DAYS_BIRTH  DAYS_EMPLOYED            NAME_EDUCATION_TYPE  DAYS_ID_PUBLISH CODE_GENDER  AMT_ANNUITY  DAYS_REGISTRATION  AMT_GOODS_PRICE  AMT_CREDIT ORGANIZATION_TYPE  DAYS_LAST_PHONE_CHANGE NAME_INCOME_TYPE  AMT_INCOME_TOTAL  OWN_CAR_AGE  TARGET\n",
            "0      0.311267      0.622246           NaN      -16765          -1188               Higher education             -291           F      35698.5            -1186.0        1129500.0   1293502.5            School                  -828.0    State servant          270000.0          NaN       0\n",
            "1           NaN      0.322738           NaN      -19932          -3038  Secondary / secondary special            -3458           M      21865.5            -4311.0         513000.0    513000.0          Religion                 -1106.0          Working          121500.0          NaN       0\n",
            "2           NaN      0.354225      0.621226      -16941          -1588  Secondary / secondary special             -477           M      27517.5            -4970.0         454500.0    490495.5             Other                 -2536.0    State servant           99000.0          NaN       0\n"
          ]
        }
      ],
      "source": [
        "# Select the same columns as in the original implementation using pandas-like syntax\n",
        "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
        "                  'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
        "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "                  'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
        "                  'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
        "\n",
        "# Pandas-like column selection - much cleaner than PySpark select()\n",
        "df = df_full[columns_extract]\n",
        "\n",
        "print(f\"Selected features shape: {df.shape}\")\n",
        "print(f\"Selected columns: {len(columns_extract)}\")\n",
        "\n",
        "print(\"\\nFirst 3 rows of selected features:\")\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-exploration"
      },
      "source": [
        "## 6. Data Exploration with Koalas\n",
        "Use pandas-like methods for data exploration and analysis. This provides familiar and intuitive data inspection capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "data-exploration-code",
        "outputId": "66ea4e78-02b2-421c-9092-18a2cab4760f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data exploration with Koalas:\n",
            "\n",
            "Data types:\n",
            "EXT_SOURCE_1              float64\n",
            "EXT_SOURCE_2              float64\n",
            "EXT_SOURCE_3              float64\n",
            "DAYS_BIRTH                  int32\n",
            "DAYS_EMPLOYED               int32\n",
            "NAME_EDUCATION_TYPE        object\n",
            "DAYS_ID_PUBLISH             int32\n",
            "CODE_GENDER                object\n",
            "AMT_ANNUITY               float64\n",
            "DAYS_REGISTRATION         float64\n",
            "AMT_GOODS_PRICE           float64\n",
            "AMT_CREDIT                float64\n",
            "ORGANIZATION_TYPE          object\n",
            "DAYS_LAST_PHONE_CHANGE    float64\n",
            "NAME_INCOME_TYPE           object\n",
            "AMT_INCOME_TOTAL          float64\n",
            "OWN_CAR_AGE               float64\n",
            "TARGET                      int32\n",
            "dtype: object\n",
            "\n",
            "Target distribution (pandas-like value_counts):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    282686\n",
            "1     24825\n",
            "Name: TARGET, dtype: int64\n",
            "\n",
            "Basic statistics for numerical columns:\n",
            "        EXT_SOURCE_1  EXT_SOURCE_2   EXT_SOURCE_3     DAYS_BIRTH  DAYS_EMPLOYED  DAYS_ID_PUBLISH    AMT_ANNUITY  DAYS_REGISTRATION  AMT_GOODS_PRICE    AMT_CREDIT  DAYS_LAST_PHONE_CHANGE  AMT_INCOME_TOTAL    OWN_CAR_AGE         TARGET\n",
            "count  134133.000000  3.068510e+05  246546.000000  307511.000000  307511.000000    307511.000000  307499.000000      307511.000000     3.072330e+05  3.075110e+05           307510.000000      3.075110e+05  104582.000000  307511.000000\n",
            "mean        0.502130  5.143927e-01       0.510853  -16036.995067   63815.045904     -2994.202373   27108.573909       -4986.120328     5.383962e+05  5.990260e+05             -962.858788      1.687979e+05      12.061091       0.080729\n",
            "std         0.211062  1.910602e-01       0.194844    4363.988632  141275.766519      1509.450419   14493.737315        3522.886321     3.694465e+05  4.024908e+05              826.808487      2.371231e+05      11.944812       0.272419\n",
            "min         0.014568  8.173617e-08       0.000527  -25229.000000  -17912.000000     -7197.000000    1615.500000      -24672.000000     4.050000e+04  4.500000e+04            -4292.000000      2.565000e+04       0.000000       0.000000\n",
            "25%         0.333986  3.923740e-01       0.370650  -19682.000000   -2761.000000     -4299.000000   16524.000000       -7478.000000     2.385000e+05  2.700000e+05            -1569.000000      1.125000e+05       5.000000       0.000000\n",
            "50%         0.505994  5.659238e-01       0.535276  -15750.000000   -1214.000000     -3254.000000   24903.000000       -4504.000000     4.500000e+05  5.135310e+05             -757.000000      1.467719e+05       9.000000       0.000000\n",
            "75%         0.675029  6.635927e-01       0.669057  -12413.000000    -289.000000     -1720.000000   34596.000000       -2010.000000     6.795000e+05  8.086500e+05             -274.000000      2.025000e+05      15.000000       0.000000\n",
            "max         0.962693  8.549997e-01       0.896010   -7489.000000  365243.000000         0.000000  258025.500000           0.000000     4.050000e+06  4.050000e+06                0.000000      1.170000e+08      91.000000       1.000000\n",
            "\n",
            "Missing values count:\n",
            "EXT_SOURCE_1              173378\n",
            "EXT_SOURCE_2                 660\n",
            "EXT_SOURCE_3               60965\n",
            "DAYS_BIRTH                     0\n",
            "DAYS_EMPLOYED                  0\n",
            "NAME_EDUCATION_TYPE            0\n",
            "DAYS_ID_PUBLISH                0\n",
            "CODE_GENDER                    0\n",
            "AMT_ANNUITY                   12\n",
            "DAYS_REGISTRATION              0\n",
            "AMT_GOODS_PRICE              278\n",
            "AMT_CREDIT                     0\n",
            "ORGANIZATION_TYPE              0\n",
            "DAYS_LAST_PHONE_CHANGE         1\n",
            "NAME_INCOME_TYPE               0\n",
            "AMT_INCOME_TOTAL               0\n",
            "OWN_CAR_AGE               202929\n",
            "TARGET                         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore data using pandas-like methods - much more intuitive than PySpark!\n",
        "print(\"Data exploration with Koalas:\")\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nTarget distribution (pandas-like value_counts):\")\n",
        "print(df['TARGET'].value_counts())\n",
        "\n",
        "print(\"\\nBasic statistics for numerical columns:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values using pandas-like syntax\n",
        "print(\"\\nMissing values count:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train-test-split"
      },
      "source": [
        "## 7. Train-Test Split with Koalas\n",
        "Use pandas-like sampling for train-test split. While not as sophisticated as sklearn's train_test_split, it's much simpler than PySpark's randomSplit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "train-test-split-code",
        "outputId": "b6d19a0d-e417-48c0-8e85-7160840733eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating train-test split with Koalas-compatible approach...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py:5723: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Train-test split completed using random column method\n",
            "\n",
            "Training set shape: (245877, 18)\n",
            "Test set shape: (61634, 18)\n",
            "\n",
            "Training set target distribution:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    226053\n",
            "1     19824\n",
            "Name: TARGET, dtype: int64\n",
            "\n",
            "Test set target distribution:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    56633\n",
            "1     5001\n",
            "Name: TARGET, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training set class distribution (%):\n",
            "0    91.937432\n",
            "1     8.062568\n",
            "Name: TARGET, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set class distribution (%):\n",
            "0    91.885972\n",
            "1     8.114028\n",
            "Name: TARGET, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Create train-test split using Koalas-compatible method\n",
        "print(\"Creating train-test split with Koalas-compatible approach...\")\n",
        "\n",
        "try:\n",
        "    # Method 1: Use random column approach (Koalas-compatible)\n",
        "    import pyspark.sql.functions as F\n",
        "\n",
        "    # Add a random column for splitting\n",
        "    df_with_random = df.to_spark().withColumn(\"random_col\", F.rand(seed=101))\n",
        "\n",
        "    # Convert back to Koalas for pandas-like operations\n",
        "    df_random = df_with_random.to_pandas_on_spark()\n",
        "\n",
        "    # Split based on random column (80/20 split)\n",
        "    train_df_koalas = df_random[df_random['random_col'] <= 0.8].drop('random_col', axis=1)\n",
        "    test_df_koalas = df_random[df_random['random_col'] > 0.8].drop('random_col', axis=1)\n",
        "\n",
        "    print(\"✓ Train-test split completed using random column method\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Random column method failed: {e}\")\n",
        "    print(\"Falling back to simple sampling approach...\")\n",
        "\n",
        "    # Method 2: Simple fallback using numpy-style split\n",
        "    # Convert to pandas temporarily for splitting\n",
        "    df_pandas = df.to_pandas()\n",
        "    df_shuffled = df_pandas.sample(frac=1, random_state=101).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split index\n",
        "    split_idx = int(0.8 * len(df_shuffled))\n",
        "\n",
        "    # Split the data\n",
        "    train_pandas = df_shuffled[:split_idx]\n",
        "    test_pandas = df_shuffled[split_idx:]\n",
        "\n",
        "    # Convert back to Koalas if available\n",
        "    if KOALAS_AVAILABLE:\n",
        "        train_df_koalas = ps.from_pandas(train_pandas)\n",
        "        test_df_koalas = ps.from_pandas(test_pandas)\n",
        "    else:\n",
        "        train_df_koalas = train_pandas\n",
        "        test_df_koalas = test_pandas\n",
        "\n",
        "    print(\"✓ Train-test split completed using fallback method\")\n",
        "\n",
        "print(f\"\\nTraining set shape: {train_df_koalas.shape}\")\n",
        "print(f\"Test set shape: {test_df_koalas.shape}\")\n",
        "\n",
        "# Check target distribution using pandas-like syntax\n",
        "print(\"\\nTraining set target distribution:\")\n",
        "print(train_df_koalas['TARGET'].value_counts())\n",
        "\n",
        "print(\"\\nTest set target distribution:\")\n",
        "print(test_df_koalas['TARGET'].value_counts())\n",
        "\n",
        "# Calculate class distribution percentages\n",
        "train_class_dist = train_df_koalas['TARGET'].value_counts(normalize=True) * 100\n",
        "print(\"\\nTraining set class distribution (%):\")\n",
        "print(train_class_dist)\n",
        "\n",
        "test_class_dist = test_df_koalas['TARGET'].value_counts(normalize=True) * 100\n",
        "print(\"\\nTest set class distribution (%):\")\n",
        "print(test_class_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-engineering"
      },
      "source": [
        "## 8. Feature Engineering with Koalas\n",
        "Use pandas-like operations for feature engineering including dummy encoding and data preprocessing. This is significantly simpler than PySpark's StringIndexer + OneHotEncoder pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "feature-engineering-code",
        "outputId": "8da53734-e27c-4137-ef8d-2153e3a1fabf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns (4): ['NAME_EDUCATION_TYPE', 'CODE_GENDER', 'ORGANIZATION_TYPE', 'NAME_INCOME_TYPE']\n",
            "Numerical columns (13): ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH', 'AMT_ANNUITY', 'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT', 'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE']\n",
            "\n",
            "Unique values in categorical columns:\n",
            "NAME_EDUCATION_TYPE: 5 unique values\n",
            "CODE_GENDER: 3 unique values\n",
            "ORGANIZATION_TYPE: 58 unique values\n",
            "NAME_INCOME_TYPE: 8 unique values\n"
          ]
        }
      ],
      "source": [
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['NAME_EDUCATION_TYPE', 'CODE_GENDER', 'ORGANIZATION_TYPE', 'NAME_INCOME_TYPE']\n",
        "numerical_cols = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
        "                 'DAYS_ID_PUBLISH', 'AMT_ANNUITY', 'DAYS_REGISTRATION', 'AMT_GOODS_PRICE',\n",
        "                 'AMT_CREDIT', 'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE']\n",
        "\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
        "\n",
        "# Check unique values in categorical columns\n",
        "print(\"\\nUnique values in categorical columns:\")\n",
        "for col in categorical_cols:\n",
        "    unique_count = df[col].nunique()\n",
        "    print(f\"{col}: {unique_count} unique values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "one-hot-encoding",
        "outputId": "c1bc65d8-cfea-441b-e14f-6f7e47bfbc94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing one-hot encoding with Koalas get_dummies...\n",
            "Training set shape after encoding: (245877, 88)\n",
            "Test set shape after encoding: (61634, 87)\n",
            "\n",
            "Aligning columns between train and test sets...\n",
            "Aligned training set shape: (245877, 87)\n",
            "Aligned test set shape: (61634, 87)\n",
            "Number of common columns: 87\n"
          ]
        }
      ],
      "source": [
        "# One-hot encoding using pandas-like get_dummies (MUCH simpler than PySpark!)\n",
        "print(\"Performing one-hot encoding with Koalas get_dummies...\")\n",
        "\n",
        "# This single line replaces complex PySpark StringIndexer + OneHotEncoder pipeline!\n",
        "train_encoded = ps.get_dummies(train_df_koalas, columns=categorical_cols, prefix=categorical_cols)\n",
        "test_encoded = ps.get_dummies(test_df_koalas, columns=categorical_cols, prefix=categorical_cols)\n",
        "\n",
        "print(f\"Training set shape after encoding: {train_encoded.shape}\")\n",
        "print(f\"Test set shape after encoding: {test_encoded.shape}\")\n",
        "\n",
        "# Align columns between train and test (pandas-like operation)\n",
        "print(\"\\nAligning columns between train and test sets...\")\n",
        "\n",
        "# Get common columns\n",
        "train_cols = set(train_encoded.columns)\n",
        "test_cols = set(test_encoded.columns)\n",
        "common_cols = list(train_cols.intersection(test_cols))\n",
        "\n",
        "# Ensure TARGET is included\n",
        "if 'TARGET' not in common_cols:\n",
        "    common_cols.append('TARGET')\n",
        "\n",
        "# Sort columns for consistency\n",
        "common_cols.sort()\n",
        "\n",
        "train_aligned = train_encoded[common_cols]\n",
        "test_aligned = test_encoded[common_cols]\n",
        "\n",
        "print(f\"Aligned training set shape: {train_aligned.shape}\")\n",
        "print(f\"Aligned test set shape: {test_aligned.shape}\")\n",
        "print(f\"Number of common columns: {len(common_cols)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "missing-values",
        "outputId": "a35405a4-4231-49d1-d4b7-c92548eb27a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handling missing values with Koalas fillna...\n",
            "Numerical medians for imputation:\n",
            "EXT_SOURCE_1: 0.506146\n",
            "EXT_SOURCE_2: 0.565822\n",
            "EXT_SOURCE_3: 0.535276\n",
            "DAYS_BIRTH: -15741.000000\n",
            "DAYS_EMPLOYED: -1212.000000\n",
            "DAYS_ID_PUBLISH: -3256.000000\n",
            "AMT_ANNUITY: 24907.500000\n",
            "DAYS_REGISTRATION: -4498.000000\n",
            "AMT_GOODS_PRICE: 450000.000000\n",
            "AMT_CREDIT: 514777.500000\n",
            "DAYS_LAST_PHONE_CHANGE: -758.000000\n",
            "AMT_INCOME_TOTAL: 144000.000000\n",
            "OWN_CAR_AGE: 9.000000\n",
            "\n",
            "Training set shape after filling missing values: (245877, 87)\n",
            "Test set shape after filling missing values: (61634, 87)\n",
            "\n",
            "Remaining missing values in training set:\n",
            "0\n",
            "\n",
            "Remaining missing values in test set:\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Handle missing values using pandas-like fillna (much simpler than PySpark Imputer!)\n",
        "print(\"Handling missing values with Koalas fillna...\")\n",
        "\n",
        "# Calculate median for numerical columns in training set\n",
        "numerical_medians = {}\n",
        "for col in numerical_cols:\n",
        "    if col in train_aligned.columns:\n",
        "        median_val = train_aligned[col].median()\n",
        "        numerical_medians[col] = median_val\n",
        "\n",
        "print(\"Numerical medians for imputation:\")\n",
        "for col, median_val in numerical_medians.items():\n",
        "    print(f\"{col}: {median_val:.6f}\")\n",
        "\n",
        "# Fill missing values with median using pandas-like fillna\n",
        "# This is much simpler than PySpark's Imputer transformer\n",
        "train_filled = train_aligned.fillna(numerical_medians)\n",
        "test_filled = test_aligned.fillna(numerical_medians)\n",
        "\n",
        "print(f\"\\nTraining set shape after filling missing values: {train_filled.shape}\")\n",
        "print(f\"Test set shape after filling missing values: {test_filled.shape}\")\n",
        "\n",
        "# Check if there are any remaining missing values\n",
        "print(\"\\nRemaining missing values in training set:\")\n",
        "print(train_filled.isnull().sum().sum())\n",
        "\n",
        "print(\"\\nRemaining missing values in test set:\")\n",
        "print(test_filled.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyspark-conversion"
      },
      "source": [
        "## 9. Convert to PySpark DataFrame for MLlib\n",
        "Convert Koalas DataFrames to PySpark DataFrames only when necessary for machine learning with MLlib. This demonstrates the seamless integration between Koalas and PySpark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pyspark-conversion-code",
        "outputId": "f47f2c08-c2ad-45b4-e5cb-5985164309a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting Koalas DataFrames to PySpark DataFrames for MLlib...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark training DataFrame count: 245877\n",
            "PySpark test DataFrame count: 61634\n",
            "\n",
            "PySpark DataFrame schema (first 10 columns):\n",
            "AMT_ANNUITY: DoubleType()\n",
            "AMT_CREDIT: DoubleType()\n",
            "AMT_GOODS_PRICE: DoubleType()\n",
            "AMT_INCOME_TOTAL: DoubleType()\n",
            "CODE_GENDER_F: ByteType()\n",
            "CODE_GENDER_M: ByteType()\n",
            "DAYS_BIRTH: DoubleType()\n",
            "DAYS_EMPLOYED: DoubleType()\n",
            "DAYS_ID_PUBLISH: DoubleType()\n",
            "DAYS_LAST_PHONE_CHANGE: DoubleType()\n",
            "\n",
            "First 3 rows of PySpark training DataFrame:\n",
            "+-----------+----------+---------------+----------------+-------------+-------------+----------+-------------+---------------+----------------------+-----------------+------------------+------------------+------------------+-----------------------------------+------------------------------------+-------------------------------------+-----------------------------------+-------------------------------------------------+----------------------------+-------------------------------------+--------------------------------+--------------------------+------------------------------+------------------------+---------------------------+------------------------+-----------------------------+-----------------------------+----------------------+----------------------------------------+----------------------------------------+----------------------------------------+--------------------------+------------------------------+-------------------------+-----------------------------+---------------------------+----------------------------+-----------------------+-------------------------+----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+---------------------------+------------------------------+--------------------------------+--------------------------+--------------------------+------------------------+-----------------------+------------------------+------------------------+-------------------------+--------------------------+----------------------------+------------------------+--------------------------+-------------------------------------+-------------------------------+--------------------------+-------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+----------------------------+---------------------+-----------+------+\n",
            "|AMT_ANNUITY|AMT_CREDIT|AMT_GOODS_PRICE|AMT_INCOME_TOTAL|CODE_GENDER_F|CODE_GENDER_M|DAYS_BIRTH|DAYS_EMPLOYED|DAYS_ID_PUBLISH|DAYS_LAST_PHONE_CHANGE|DAYS_REGISTRATION|EXT_SOURCE_1      |EXT_SOURCE_2      |EXT_SOURCE_3      |NAME_EDUCATION_TYPE_Academic degree|NAME_EDUCATION_TYPE_Higher education|NAME_EDUCATION_TYPE_Incomplete higher|NAME_EDUCATION_TYPE_Lower secondary|NAME_EDUCATION_TYPE_Secondary / secondary special|NAME_INCOME_TYPE_Businessman|NAME_INCOME_TYPE_Commercial associate|NAME_INCOME_TYPE_Maternity leave|NAME_INCOME_TYPE_Pensioner|NAME_INCOME_TYPE_State servant|NAME_INCOME_TYPE_Student|NAME_INCOME_TYPE_Unemployed|NAME_INCOME_TYPE_Working|ORGANIZATION_TYPE_Advertising|ORGANIZATION_TYPE_Agriculture|ORGANIZATION_TYPE_Bank|ORGANIZATION_TYPE_Business Entity Type 1|ORGANIZATION_TYPE_Business Entity Type 2|ORGANIZATION_TYPE_Business Entity Type 3|ORGANIZATION_TYPE_Cleaning|ORGANIZATION_TYPE_Construction|ORGANIZATION_TYPE_Culture|ORGANIZATION_TYPE_Electricity|ORGANIZATION_TYPE_Emergency|ORGANIZATION_TYPE_Government|ORGANIZATION_TYPE_Hotel|ORGANIZATION_TYPE_Housing|ORGANIZATION_TYPE_Industry: type 1|ORGANIZATION_TYPE_Industry: type 10|ORGANIZATION_TYPE_Industry: type 11|ORGANIZATION_TYPE_Industry: type 12|ORGANIZATION_TYPE_Industry: type 13|ORGANIZATION_TYPE_Industry: type 2|ORGANIZATION_TYPE_Industry: type 3|ORGANIZATION_TYPE_Industry: type 4|ORGANIZATION_TYPE_Industry: type 5|ORGANIZATION_TYPE_Industry: type 6|ORGANIZATION_TYPE_Industry: type 7|ORGANIZATION_TYPE_Industry: type 8|ORGANIZATION_TYPE_Industry: type 9|ORGANIZATION_TYPE_Insurance|ORGANIZATION_TYPE_Kindergarten|ORGANIZATION_TYPE_Legal Services|ORGANIZATION_TYPE_Medicine|ORGANIZATION_TYPE_Military|ORGANIZATION_TYPE_Mobile|ORGANIZATION_TYPE_Other|ORGANIZATION_TYPE_Police|ORGANIZATION_TYPE_Postal|ORGANIZATION_TYPE_Realtor|ORGANIZATION_TYPE_Religion|ORGANIZATION_TYPE_Restaurant|ORGANIZATION_TYPE_School|ORGANIZATION_TYPE_Security|ORGANIZATION_TYPE_Security Ministries|ORGANIZATION_TYPE_Self-employed|ORGANIZATION_TYPE_Services|ORGANIZATION_TYPE_Telecom|ORGANIZATION_TYPE_Trade: type 1|ORGANIZATION_TYPE_Trade: type 2|ORGANIZATION_TYPE_Trade: type 3|ORGANIZATION_TYPE_Trade: type 4|ORGANIZATION_TYPE_Trade: type 5|ORGANIZATION_TYPE_Trade: type 6|ORGANIZATION_TYPE_Trade: type 7|ORGANIZATION_TYPE_Transport: type 1|ORGANIZATION_TYPE_Transport: type 2|ORGANIZATION_TYPE_Transport: type 3|ORGANIZATION_TYPE_Transport: type 4|ORGANIZATION_TYPE_University|ORGANIZATION_TYPE_XNA|OWN_CAR_AGE|TARGET|\n",
            "+-----------+----------+---------------+----------------+-------------+-------------+----------+-------------+---------------+----------------------+-----------------+------------------+------------------+------------------+-----------------------------------+------------------------------------+-------------------------------------+-----------------------------------+-------------------------------------------------+----------------------------+-------------------------------------+--------------------------------+--------------------------+------------------------------+------------------------+---------------------------+------------------------+-----------------------------+-----------------------------+----------------------+----------------------------------------+----------------------------------------+----------------------------------------+--------------------------+------------------------------+-------------------------+-----------------------------+---------------------------+----------------------------+-----------------------+-------------------------+----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+---------------------------+------------------------------+--------------------------------+--------------------------+--------------------------+------------------------+-----------------------+------------------------+------------------------+-------------------------+--------------------------+----------------------------+------------------------+--------------------------+-------------------------------------+-------------------------------+--------------------------+-------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+----------------------------+---------------------+-----------+------+\n",
            "|35698.5    |1293502.5 |1129500.0      |270000.0        |1            |0            |-16765.0  |-1188.0      |-291.0         |-828.0                |-1186.0          |0.3112673113812225|0.6222457752555098|0.5352762504724826|0                                  |1                                   |0                                    |0                                  |0                                                |0                           |0                                    |0                               |0                         |1                             |0                       |0                          |0                       |0                            |0                            |0                     |0                                       |0                                       |0                                       |0                         |0                             |0                        |0                            |0                          |0                           |0                      |0                        |0                                 |0                                  |0                                  |0                                  |0                                  |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                          |0                             |0                               |0                         |0                         |0                       |0                      |0                       |0                       |0                        |0                         |0                           |1                       |0                         |0                                    |0                              |0                         |0                        |0                              |0                              |0                              |0                              |0                              |0                              |0                              |0                                  |0                                  |0                                  |0                                  |0                           |0                    |9.0        |0     |\n",
            "|21865.5    |513000.0  |513000.0       |121500.0        |0            |1            |-19932.0  |-3038.0      |-3458.0        |-1106.0               |-4311.0          |0.5061459704887005|0.3227382869704046|0.5352762504724826|0                                  |0                                   |0                                    |0                                  |1                                                |0                           |0                                    |0                               |0                         |0                             |0                       |0                          |1                       |0                            |0                            |0                     |0                                       |0                                       |0                                       |0                         |0                             |0                        |0                            |0                          |0                           |0                      |0                        |0                                 |0                                  |0                                  |0                                  |0                                  |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                          |0                             |0                               |0                         |0                         |0                       |0                      |0                       |0                       |0                        |1                         |0                           |0                       |0                         |0                                    |0                              |0                         |0                        |0                              |0                              |0                              |0                              |0                              |0                              |0                              |0                                  |0                                  |0                                  |0                                  |0                           |0                    |9.0        |0     |\n",
            "|27517.5    |490495.5  |454500.0       |99000.0         |0            |1            |-16941.0  |-1588.0      |-477.0         |-2536.0               |-4970.0          |0.5061459704887005|0.3542247319929012|0.6212263380626669|0                                  |0                                   |0                                    |0                                  |1                                                |0                           |0                                    |0                               |0                         |1                             |0                       |0                          |0                       |0                            |0                            |0                     |0                                       |0                                       |0                                       |0                         |0                             |0                        |0                            |0                          |0                           |0                      |0                        |0                                 |0                                  |0                                  |0                                  |0                                  |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                                 |0                          |0                             |0                               |0                         |0                         |0                       |1                      |0                       |0                       |0                        |0                         |0                           |0                       |0                         |0                                    |0                              |0                         |0                        |0                              |0                              |0                              |0                              |0                              |0                              |0                              |0                                  |0                                  |0                                  |0                                  |0                           |0                    |9.0        |0     |\n",
            "+-----------+----------+---------------+----------------+-------------+-------------+----------+-------------+---------------+----------------------+-----------------+------------------+------------------+------------------+-----------------------------------+------------------------------------+-------------------------------------+-----------------------------------+-------------------------------------------------+----------------------------+-------------------------------------+--------------------------------+--------------------------+------------------------------+------------------------+---------------------------+------------------------+-----------------------------+-----------------------------+----------------------+----------------------------------------+----------------------------------------+----------------------------------------+--------------------------+------------------------------+-------------------------+-----------------------------+---------------------------+----------------------------+-----------------------+-------------------------+----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+----------------------------------+---------------------------+------------------------------+--------------------------------+--------------------------+--------------------------+------------------------+-----------------------+------------------------+------------------------+-------------------------+--------------------------+----------------------------+------------------------+--------------------------+-------------------------------------+-------------------------------+--------------------------+-------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+----------------------------+---------------------+-----------+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert Koalas DataFrames to PySpark DataFrames for MLlib\n",
        "print(\"Converting Koalas DataFrames to PySpark DataFrames for MLlib...\")\n",
        "\n",
        "# Convert to PySpark DataFrames using .to_spark()\n",
        "train_spark = train_filled.to_spark()\n",
        "test_spark = test_filled.to_spark()\n",
        "\n",
        "print(f\"PySpark training DataFrame count: {train_spark.count()}\")\n",
        "print(f\"PySpark test DataFrame count: {test_spark.count()}\")\n",
        "\n",
        "# Show schema\n",
        "print(\"\\nPySpark DataFrame schema (first 10 columns):\")\n",
        "for field in train_spark.schema.fields[:10]:\n",
        "    print(f\"{field.name}: {field.dataType}\")\n",
        "\n",
        "print(\"\\nFirst 3 rows of PySpark training DataFrame:\")\n",
        "train_spark.show(3, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-preparation"
      },
      "source": [
        "## 10. Feature Preparation for MLlib\n",
        "Prepare features for PySpark MLlib using VectorAssembler. This is the only part where we need to use PySpark-specific operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "feature-preparation-code",
        "outputId": "3f7c48bb-3133-4e52-9411-5bbaf6d33865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing features for MLlib...\n",
            "Number of feature columns: 86\n",
            "Training data with features vector: 245885 rows\n",
            "Test data with features vector: 61626 rows\n",
            "\n",
            "Final training data schema:\n",
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- TARGET: integer (nullable = true)\n",
            "\n",
            "\n",
            "Sample of assembled features:\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
            "|features                                                                                                                                                                                                   |TARGET|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
            "|(86,[0,1,2,3,4,6,7,8,9,10,11,12,13,15,23,66,85],[35698.5,1293502.5,1129500.0,270000.0,1.0,-16765.0,-1188.0,-291.0,-828.0,-1186.0,0.3112673113812225,0.6222457752555098,0.5352762504724826,1.0,1.0,1.0,9.0])|0     |\n",
            "|(86,[0,1,2,3,5,6,7,8,9,10,11,12,13,18,26,64,85],[21865.5,513000.0,513000.0,121500.0,1.0,-19932.0,-3038.0,-3458.0,-1106.0,-4311.0,0.5061459704887005,0.3227382869704046,0.5352762504724826,1.0,1.0,1.0,9.0])|0     |\n",
            "|(86,[0,1,2,3,5,6,7,8,9,10,11,12,13,18,23,60,85],[27517.5,490495.5,454500.0,99000.0,1.0,-16941.0,-1588.0,-477.0,-2536.0,-4970.0,0.5061459704887005,0.3542247319929012,0.6212263380626669,1.0,1.0,1.0,9.0])  |0     |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare features for MLlib using VectorAssembler\n",
        "print(\"Preparing features for MLlib...\")\n",
        "\n",
        "# Get feature columns (all columns except TARGET)\n",
        "feature_cols = [col for col in train_spark.columns if col != 'TARGET']\n",
        "print(f\"Number of feature columns: {len(feature_cols)}\")\n",
        "\n",
        "# Create VectorAssembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"  # Skip rows with invalid values\n",
        ")\n",
        "\n",
        "# Transform training and test data\n",
        "train_assembled = assembler.transform(train_spark)\n",
        "test_assembled = assembler.transform(test_spark)\n",
        "\n",
        "print(f\"Training data with features vector: {train_assembled.count()} rows\")\n",
        "print(f\"Test data with features vector: {test_assembled.count()} rows\")\n",
        "\n",
        "# Select only the features and target columns for training\n",
        "train_final = train_assembled.select(\"features\", \"TARGET\")\n",
        "test_final = test_assembled.select(\"features\", \"TARGET\")\n",
        "\n",
        "print(\"\\nFinal training data schema:\")\n",
        "train_final.printSchema()\n",
        "\n",
        "print(\"\\nSample of assembled features:\")\n",
        "train_final.show(3, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-training"
      },
      "source": [
        "## 11. Random Forest Model Training\n",
        "Train the Random Forest model using PySpark MLlib. This is where we must use PySpark since Koalas doesn't provide machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "model-training-code",
        "outputId": "c1db1331-6b5f-4ad2-c436-36bc0528f1c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Random Forest model...\n",
            "Fitting Random Forest model...\n",
            "Model training completed!\n",
            "Number of trees: 100\n",
            "Feature importance vector size: 86\n"
          ]
        }
      ],
      "source": [
        "# Train Random Forest model using PySpark MLlib\n",
        "print(\"Training Random Forest model...\")\n",
        "\n",
        "# Create Random Forest classifier with same parameters as original\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"TARGET\",\n",
        "    numTrees=100,  # Same as n_estimators=100 in sklearn\n",
        "    seed=101\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Fitting Random Forest model...\")\n",
        "rf_model = rf.fit(train_final)\n",
        "\n",
        "print(\"Model training completed!\")\n",
        "print(f\"Number of trees: {rf_model.getNumTrees}\")\n",
        "print(f\"Feature importance vector size: {len(rf_model.featureImportances)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-evaluation"
      },
      "source": [
        "## 12. Model Evaluation\n",
        "Evaluate the model using various metrics including accuracy, F1-score, and AUC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "model-evaluation-code",
        "outputId": "99cd978c-2b35-4905-b377-2b3b481602fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making predictions on test set...\n",
            "\n",
            "Sample predictions:\n",
            "+------+----------+--------------------+\n",
            "|TARGET|prediction|         probability|\n",
            "+------+----------+--------------------+\n",
            "|     0|       0.0|[0.92259315023424...|\n",
            "|     0|       0.0|[0.92688126014464...|\n",
            "|     0|       0.0|[0.92403199026644...|\n",
            "|     0|       0.0|[0.92001367924317...|\n",
            "|     0|       0.0|[0.91704108296053...|\n",
            "|     0|       0.0|[0.92536513722143...|\n",
            "|     0|       0.0|[0.91791817914322...|\n",
            "|     0|       0.0|[0.91981700926013...|\n",
            "|     0|       0.0|[0.90150008271025...|\n",
            "|     0|       0.0|[0.91836415221207...|\n",
            "+------+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "Calculating evaluation metrics...\n",
            "\n",
            "=== MODEL EVALUATION RESULTS ===\n",
            "Accuracy: 0.919044\n",
            "F1-Score: 0.880273\n",
            "AUC: 0.716582\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on test set\n",
        "print(\"Making predictions on test set...\")\n",
        "predictions = rf_model.transform(test_final)\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\nSample predictions:\")\n",
        "predictions.select(\"TARGET\", \"prediction\", \"probability\").show(10)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "print(\"\\nCalculating evaluation metrics...\")\n",
        "\n",
        "# Accuracy\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"TARGET\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "\n",
        "# F1 Score\n",
        "f1_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"TARGET\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"f1\"\n",
        ")\n",
        "f1_score = f1_evaluator.evaluate(predictions)\n",
        "\n",
        "# AUC\n",
        "auc_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"TARGET\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "auc = auc_evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"\\n=== MODEL EVALUATION RESULTS ===\")\n",
        "print(f\"Accuracy: {accuracy:.6f}\")\n",
        "print(f\"F1-Score: {f1_score:.6f}\")\n",
        "print(f\"AUC: {auc:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question-4-2"
      },
      "source": [
        "## 13. Question 4.2: Is Accuracy a Good Metric for This Problem?\n",
        "\n",
        "**Answer: No, accuracy is NOT a good choice of metric for this problem.**\n",
        "\n",
        "### Reasons why accuracy is inappropriate:\n",
        "\n",
        "1. **Severe Class Imbalance**: The dataset shows a significant class imbalance with approximately 91.9% of samples belonging to class 0 (no default) and only 8.1% belonging to class 1 (default). In such imbalanced scenarios, accuracy can be misleading.\n",
        "\n",
        "2. **Accuracy Paradox**: A naive classifier that always predicts the majority class (no default) would achieve ~92% accuracy without learning anything meaningful about the problem. This demonstrates the \"accuracy paradox\" where high accuracy doesn't necessarily indicate good model performance.\n",
        "\n",
        "3. **Business Impact**: In credit risk assessment, the cost of false negatives (missing actual defaults) is typically much higher than false positives (incorrectly flagging good customers). Accuracy treats both types of errors equally.\n",
        "\n",
        "4. **Lack of Insight**: Accuracy doesn't provide information about the model's ability to identify the minority class (defaults), which is the primary objective in this credit risk problem.\n",
        "\n",
        "### Better Metrics for This Problem:\n",
        "\n",
        "1. **Precision**: Measures the proportion of predicted defaults that are actual defaults\n",
        "2. **Recall (Sensitivity)**: Measures the proportion of actual defaults that are correctly identified\n",
        "3. **F1-Score**: Harmonic mean of precision and recall, providing a balanced measure\n",
        "4. **AUC-ROC**: Measures the model's ability to distinguish between classes across all thresholds\n",
        "5. **Precision-Recall AUC**: Particularly useful for imbalanced datasets\n",
        "6. **Business-specific metrics**: Such as expected loss or profit-based evaluation\n",
        "\n",
        "### Conclusion:\n",
        "For this credit default prediction problem, metrics like F1-score, AUC, precision, and recall provide much more meaningful insights into model performance than accuracy alone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-saving"
      },
      "source": [
        "## 14. Model Saving\n",
        "Save the trained Random Forest model to disk for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "model-saving-code",
        "outputId": "0ebafe8e-9a46-4915-9921-916fcbc9203b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: ./koalas_random_forest_model\n",
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "model_path = \"./koalas_random_forest_model\"\n",
        "print(f\"Saving model to: {model_path}\")\n",
        "\n",
        "try:\n",
        "    rf_model.write().overwrite().save(model_path)\n",
        "    print(\"Model saved successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")\n",
        "    # Alternative: save to a different path\n",
        "    import tempfile\n",
        "    import os\n",
        "    temp_path = os.path.join(tempfile.gettempdir(), \"koalas_rf_model\")\n",
        "    rf_model.write().overwrite().save(temp_path)\n",
        "    print(f\"Model saved to temporary location: {temp_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-importance"
      },
      "source": [
        "## 15. Feature Importance Analysis\n",
        "Analyze and display the most important features identified by the Random Forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "feature-importance-code",
        "outputId": "bd5fa868-c2b3-4bf3-80c1-b52ddd45a3b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing feature importances...\n",
            "\n",
            "Top 15 Most Important Features:\n",
            "                                              feature  importance\n",
            "13                                       EXT_SOURCE_3    0.218866\n",
            "11                                       EXT_SOURCE_1    0.210531\n",
            "12                                       EXT_SOURCE_2    0.135659\n",
            "15               NAME_EDUCATION_TYPE_Higher education    0.059525\n",
            "4                                       CODE_GENDER_F    0.057114\n",
            "26                           NAME_INCOME_TYPE_Working    0.049414\n",
            "18  NAME_EDUCATION_TYPE_Secondary / secondary special    0.037667\n",
            "7                                       DAYS_EMPLOYED    0.035960\n",
            "6                                          DAYS_BIRTH    0.026908\n",
            "2                                     AMT_GOODS_PRICE    0.018784\n",
            "5                                       CODE_GENDER_M    0.018425\n",
            "85                                        OWN_CAR_AGE    0.015661\n",
            "10                                  DAYS_REGISTRATION    0.014688\n",
            "22                         NAME_INCOME_TYPE_Pensioner    0.014155\n",
            "9                              DAYS_LAST_PHONE_CHANGE    0.012333\n",
            "\n",
            "Feature Importance Statistics:\n",
            "Total features: 86\n",
            "Sum of importances: 1.000000\n",
            "Mean importance: 0.011628\n",
            "Max importance: 0.218866\n",
            "Min importance: 0.000000\n"
          ]
        }
      ],
      "source": [
        "# Extract and display feature importances\n",
        "print(\"Analyzing feature importances...\")\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf_model.featureImportances.toArray()\n",
        "\n",
        "# Create feature importance DataFrame using Koalas (pandas-like syntax)\n",
        "feature_importance_data = list(zip(feature_cols, importances))\n",
        "feature_importance_df = ps.DataFrame(feature_importance_data, columns=['feature', 'importance'])\n",
        "\n",
        "# Sort by importance (pandas-like syntax)\n",
        "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features:\")\n",
        "print(feature_importance_df.head(15))\n",
        "\n",
        "# Show summary statistics\n",
        "print(\"\\nFeature Importance Statistics:\")\n",
        "print(f\"Total features: {len(feature_cols)}\")\n",
        "print(f\"Sum of importances: {importances.sum():.6f}\")\n",
        "print(f\"Mean importance: {importances.mean():.6f}\")\n",
        "print(f\"Max importance: {importances.max():.6f}\")\n",
        "print(f\"Min importance: {importances.min():.6f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}